{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining for Business Analytics\n",
    "\n",
    "## Similarity, Neighbors\n",
    "\n",
    "Spring 2019 - Prof. George Valkanas\n",
    "\n",
    "Material based on content courtesy of Prof. Foster Provost\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we will be using\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.spatial import distance\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "plt.rcParams['figure.figsize'] = 10, 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Wine Recommendation\n",
    "\n",
    "**Customer:** I'd like to order the _Cabernet Sauvignon_ \n",
    "\n",
    "**Waiter:** We're just out of this wine.\n",
    "\n",
    "**Customer:** What would be _the closest_ wine that you currently have?\n",
    "\n",
    "**Waiter:** We have some very nice _Merlot_ or you could try the _Malbec-Cabernet_\n",
    "\n",
    "<br/>\n",
    "\n",
    "#### <div style=\"color: red\">Is there a <u>target variable</u> that we can use?</div>\n",
    "**ANS:**\n",
    "no, since we're just looking for similar things, but not specific features.\n",
    "    \n",
    "#### <div style=\"color: red\">How did the waiter know about this recommendation?</div>\n",
    "**ANS:**\n",
    "Some features include _target customers, color, price range, origin, type of grapes / flavor_\n",
    "\n",
    "#### <div style=\"color: red\">Did she/he know that the customer _will like_ the suggestion? How would _you_ make such a recommendation ?</div>\n",
    "**ANS:**\n",
    "The waiter does not know for sure that the customer will like it. However, given that the customer likes a type of wine (Cabernet Sauvignon in this case), there is a high possibility that the customer may like it -- so why not give it a try to suggest it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Plagiarism Detection / Near-Duplicate Detection in Web Search\n",
    "\n",
    "\n",
    "Plagiarism is not just an unethical practice, but can also have legal ramifications. Plagiarism arises when someone (a person, an entity) copies someone else's (literary) work (without the proper attribution) and tries to pass it as their own.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: right\"><i>\"When you steal from one author, it's plagiarism; if you steal from many, it's research.\"<br/> Wilson Mizner</i>\n",
    "</div>\n",
    "\n",
    "\n",
    "Meanwhile, when you search the web with your favorite search engine, you'd like to get back results, each of which conveys _new_ information. That is, if all of the first 10 results **all** present **the same** information, you'd not be very happy about it. The first link will have new information (and answer your question), the second one may just reinforce what you learned from the 1st link, but after that it's just more of the same.\n",
    "\n",
    "\n",
    "Notice that these two problems are effetively the same. The way that the results are used is different in the two scenarios.\n",
    "\n",
    "<br/>\n",
    "\n",
    "#### <div style=\"color: red\">How can we solve this problem?</div>\n",
    "\n",
    "**ANS:**\n",
    "Use tf-idf \n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Player Substitution\n",
    "\n",
    "Imagine that you're playing your favorite video game (football, basketball, etc) and one of the players gets injured and needs to be taken off the court / field. \n",
    "\n",
    "\n",
    "#### <div style=\"color: red\">Is there a <u>target variable</u> that we can use?</div>\n",
    "**ANS:**\n",
    "no, since we're just looking for similar things, but not specific features.\n",
    "\n",
    "\n",
    "#### <div style=\"color: red\">Which player do you replace them with?</div>\n",
    "**ANS:**\n",
    "Similar to the wine problem, but with players... Swap the ones that are most similar in terms of features.\n",
    "Some features include: Position, stamina, experience, speed, etc.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fundamental Question\n",
    "\n",
    "We have seen various classifiers until now. The main question to answer here is the following:\n",
    "\n",
    "**<div style=\"text-align: center\">Do we have a target variable that we can use?</div>** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No. Because for all of the problems above, we're simply comparing without having a specific feature.\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Show me your friends..._\n",
    "\n",
    "Unlike magnets, _similars_ attract when it comes to people. That is, our friends' interests are more aligned with our own. Another way to see this is that we may tend to befriend individuals who have _common interests_ with our own.\n",
    "\n",
    "In abstract terms, we are _more similar_ with people we are friends with and _less similar_ with people who we (knowingly / decidedly) aren't friends with.\n",
    "\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are you talking about?\n",
    "\n",
    "Similarity between **people**? Between **wines**? Between **texts**? What are you talking about? How can these things be possibly related?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's start simple\n",
    "\n",
    "- Can you determine the similarity between two integers? For example, which two numbers would you say are _more similar_? Numbers 9 and 10,  or the numbers 1 and 100?\n",
    "    * What about negative numbers? How about -3 and -4 compared against -100 and -98?\n",
    "\n",
    "\n",
    "- Can you come up with a similarity measure for two real-valued numbers? Is it any different from integer values?\n",
    "\n",
    "\n",
    "- Can you measure the similarity between GPS signals? A GPS signal is a pair of (x, y) coordinates, commonly refered to as _latitude_ and _longitude_. Let's say that you are given 3 pairs of such GPS coordinates:\n",
    "    * **GPS 1:** (1, 1)\n",
    "    * **GPS 2:** (1, 0.5)\n",
    "    * **GPS 3:** (2, 2)\n",
    "    \n",
    "  Which pair of GPS coordinates is _more similar_ ? How did you compute the similarity? <br>\n",
    "***Note differences between Euclidean Distance (stragiht line between 2 pts) & Manhattan Distance (grid)**\n",
    "<br>\n",
    "\n",
    "**ANS:**\n",
    "In this case, we use the Manhattan Distance instead of the Euclidean Distance\n",
    "\n",
    "\n",
    "- OK, let's go for something more involved. Imagine that you are given 3 baskets with fruits:\n",
    "    * **Basket 1:** Fruit_1, Fruit_2, Fruit_3\n",
    "    * **Basket 2:** Fruit_1, Fruit_2, Fruit_4, Fruit_5\n",
    "    * **Basket 3:** Fruit_3, Fruit_2\n",
    "    \n",
    "  What if our basket had multiples of the same fruit, e.g., **Bakset 1:** Fruit_1, Fruit_1, Fruit_2, Fruit_3 ?\n",
    "    \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apr. 10th, 2019\n",
    "### Back to the difficult cases\n",
    "\n",
    "How do you define the similarity between:\n",
    "\n",
    "- Two _wines_?\n",
    "- Two _pieces of text_ ?\n",
    "- Two _people_ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalizing from our previous examples, we are discussing about the similarity of _objects_. \n",
    "\n",
    "**Question:** Did we use just a single similarity?\n",
    "\n",
    "In case it's also not obvious by now, **to compute the similarity between _objects_ we need to have quantifiable _features_**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enough talk! Let's work with some Data\n",
    "\n",
    "Following that discussion, we have compiled a scotch whiskey data set. You can find it in `data/scotch.csv`.\n",
    "\n",
    "The data consists of 5 general whiskey attributes, each of which has many possible values:\n",
    "\n",
    "- **Color**: yellow, very pale, pale, pale gold, gold, old gold, full gold, amber, etc.\n",
    "- **Nose**: aromatic, peaty, sweet, light, fresh, dry, grassy, etc.\n",
    "- **Body**: soft, medium, full, round, smooth, light, firm, oily.\n",
    "- **Palate**: full, dry, sherry, big, fruity, grassy, smoky, salty, etc.\n",
    "- **Finish**: full, dry, warm, light, smooth, clean, fruity, grassy, smoky, etc.\n",
    "\n",
    "Let's read the file in and take a look. For convenience, we have also dummysized the data. There are a few other features unrelated to the ones above. For this class, we will be dropping them. However, feel free to check them out!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/scotch.csv\")\n",
    "\n",
    "data = data.drop([u'age', u'dist', u'score', u'percent', u'region', u'district', u'islay', u'midland', u'spey', u'east', u'west', u'north ', u'lowland', u'campbell', u'islands'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color.wine</th>\n",
       "      <th>color.yellow</th>\n",
       "      <th>color.v.pale</th>\n",
       "      <th>color.pale</th>\n",
       "      <th>color.p.gold</th>\n",
       "      <th>color.gold</th>\n",
       "      <th>color.o.gold</th>\n",
       "      <th>color.f.gold</th>\n",
       "      <th>color.bronze</th>\n",
       "      <th>color.p.amber</th>\n",
       "      <th>...</th>\n",
       "      <th>fin.smoke</th>\n",
       "      <th>fin.sweet</th>\n",
       "      <th>fin.spice</th>\n",
       "      <th>fin.oil</th>\n",
       "      <th>fin.salt</th>\n",
       "      <th>fin.arome</th>\n",
       "      <th>fin.ling</th>\n",
       "      <th>fin.long</th>\n",
       "      <th>fin.very</th>\n",
       "      <th>fin.quick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Aberfeldy</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aberlour</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ardberg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ardmore</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Auchentoshan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              color.wine  color.yellow  color.v.pale  color.pale  \\\n",
       "Aberfeldy              0             1             0           0   \n",
       "Aberlour               0             0             0           0   \n",
       "Ardberg                0             0             0           0   \n",
       "Ardmore                0             0             0           0   \n",
       "Auchentoshan           0             0             0           0   \n",
       "\n",
       "              color.p.gold  color.gold  color.o.gold  color.f.gold  \\\n",
       "Aberfeldy                0           0             0             0   \n",
       "Aberlour                 0           0             0             0   \n",
       "Ardberg                  0           0             0             0   \n",
       "Ardmore                  1           0             0             0   \n",
       "Auchentoshan             1           0             0             0   \n",
       "\n",
       "              color.bronze  color.p.amber    ...      fin.smoke  fin.sweet  \\\n",
       "Aberfeldy                0              0    ...              0          0   \n",
       "Aberlour                 0              0    ...              0          0   \n",
       "Ardberg                  0              0    ...              0          0   \n",
       "Ardmore                  0              0    ...              0          0   \n",
       "Auchentoshan             0              0    ...              0          0   \n",
       "\n",
       "              fin.spice  fin.oil  fin.salt  fin.arome  fin.ling  fin.long  \\\n",
       "Aberfeldy             1        0         0          0         0         0   \n",
       "Aberlour              0        0         0          0         1         0   \n",
       "Ardberg               0        0         1          0         0         0   \n",
       "Ardmore               0        0         0          0         0         0   \n",
       "Auchentoshan          1        0         0          0         0         0   \n",
       "\n",
       "              fin.very  fin.quick  \n",
       "Aberfeldy            0          0  \n",
       "Aberlour             0          0  \n",
       "Ardberg              0          0  \n",
       "Ardmore              0          0  \n",
       "Auchentoshan         0          0  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We've discussed earlier that there are many similarity measures.  Similarity is often cast as \"closeness\" in some space, computed by a distance measure.  Often in data science, the terms similarity and distance are used interchangeably (a little strangely to the uninitiated). \n",
    "\n",
    "We'll use the library scipy.spatial.distance available [here](http://docs.scipy.org/doc/scipy/reference/spatial.distance.html)\n",
    "\n",
    "This library has functions to compute the distance between two **numeric** vectors. In particular, **pdist(X[, metric, p, w, V, VI])**\tcomputes pairwise distances between the observations in n-dimensional space.\n",
    "\n",
    "The _metric_ parameter refers to the **distance function** used to compute the distance between the instances (observations) and can be:\n",
    "* ‘braycurtis’, \n",
    "* ‘canberra’, \n",
    "* ‘chebyshev’, \n",
    "* ‘cityblock’, \n",
    "* ‘correlation’, \n",
    "* ‘cosine’, \n",
    "* ‘dice’, \n",
    "* ‘euclidean’, \n",
    "* ‘hamming’, \n",
    "* ‘jaccard’, \n",
    "* ‘kulsinski’, \n",
    "* ‘mahalanobis’, \n",
    "* ‘matching’, \n",
    "* ‘minkowski’, \n",
    "* ‘rogerstanimoto’, \n",
    "* ‘russellrao’, \n",
    "* ‘seuclidean’, \n",
    "* ‘sokalmichener’, \n",
    "* ‘sokalsneath’, \n",
    "* ‘sqeuclidean’, \n",
    "* ‘yule’\n",
    "\n",
    "Before diving in deep, let's see how the method actually works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.19615242, 10.39230485,  5.19615242])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Euclidean Distance\n",
    "sample_data = [ [1,2,3], [4,5,6], [7,8,9] ]\n",
    "distance.pdist(sample_data, 'euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, having all of that in the same row isn't particularly helpful. What are we really looking at here?\n",
    "Let's better format our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  5.19615242, 10.39230485],\n",
       "       [ 5.19615242,  0.        ,  5.19615242],\n",
       "       [10.39230485,  5.19615242,  0.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Euclidean Distance\n",
    "sample_data = [ [1,2,3], [4,5,6], [7,8,9] ]\n",
    "distance.squareform(distance.pdist(sample_data, 'euclidean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note the diff between Jaccard similarity matrix & the correlation matrix (1 on the diagonal instead of 0).** <br>\n",
    "OK, that's better. Let's do the same for the other distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.33333333, 0.75      ],\n",
       "       [0.33333333, 0.        , 0.66666667],\n",
       "       [0.75      , 0.66666667, 0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What about Jaccard distance\n",
    "sample_data = [ [0,0,1,1,1], [0,0,1,0,1], [1,0,0,0,1] ]\n",
    "distance.squareform(distance.pdist(sample_data, 'jaccard'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that the resulted array is in the row space of the sample_data matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1]\n",
      " [0 0 1 0 1]\n",
      " [1 0 0 0 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([0,0,1,1,1,0,0,1,0,1,1,0,0,0,1])\n",
    "b = a.reshape((3, 5))\n",
    "print(b)\n",
    "LA.norm(b, np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0.2, 0.6],\n",
       "       [0.2, 0. , 0.4],\n",
       "       [0.6, 0.4, 0. ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What about Hamming distance\n",
    "sample_data = [ [0,0,1,1,1], [0,0,1,0,1], [1,0,0,0,1] ]\n",
    "distance.squareform(distance.pdist(sample_data, 'hamming'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Alright, now that we got the hang of it, let's make our life easier as follows.  Here is a function that will compute a number of distances of interest that we will provide ( $distance_measures$ parameter )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def whiskey_distance(whiskey_name, distance_measures, k):\n",
    "    \n",
    "    \"\"\"\n",
    "    whiskey_name: The name of the whiskey to search for\n",
    "    distance_measures: A list containing the different distances that we want to compute\n",
    "    k: The number of similar objects that we want to retrieve\n",
    "    \"\"\"\n",
    "    \n",
    "    # We want a data frame to store the output\n",
    "    # distance_measures is a list of the distance measures you want to compute (see below)\n",
    "    # n is how many \"most similar\" to report\n",
    "    distances = pd.DataFrame()\n",
    "    \n",
    "    # Find the location of the whiskey we are looking for\n",
    "    whiskey_location = np.where(data.index == whiskey_name)[0][0]\n",
    "\n",
    "    # Go through all distance measures we care about\n",
    "    for distance_measure in distance_measures:\n",
    "        # Find all pairwise distances\n",
    "        current_distances = distance.squareform(distance.pdist(data, distance_measure))\n",
    "        # Get the closest n elements for the whiskey we care about\n",
    "        most_similar = np.argsort(current_distances[:, whiskey_location])[0:k]\n",
    "        # Append results (a new column to the dataframe with the name of the measure)\n",
    "        distances[distance_measure] = list(zip(data.index[most_similar], current_distances[most_similar, whiskey_location]))\n",
    "    return distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>euclidean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Bunnahabhain, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Glenglassaugh, 3.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Ardberg, 3.1622776601683795)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Bruichladdich, 3.1622776601683795)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Tullibardine, 3.3166247903554)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(Caperdonich, 3.4641016151377544)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             euclidean\n",
       "0                  (Bunnahabhain, 0.0)\n",
       "1                 (Glenglassaugh, 3.0)\n",
       "2        (Ardberg, 3.1622776601683795)\n",
       "3  (Bruichladdich, 3.1622776601683795)\n",
       "4      (Tullibardine, 3.3166247903554)\n",
       "5    (Caperdonich, 3.4641016151377544)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Let's test the distance of one of the whiskeys that are in there\n",
    "whiskey_distance('Bunnahabhain', ['euclidean'], 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about more distances? What is the output then?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>euclidean</th>\n",
       "      <th>cityblock</th>\n",
       "      <th>cosine</th>\n",
       "      <th>jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Bunnahabhain, 0.0)</td>\n",
       "      <td>(Bunnahabhain, 0.0)</td>\n",
       "      <td>(Bunnahabhain, 0.0)</td>\n",
       "      <td>(Bunnahabhain, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Glenglassaugh, 3.0)</td>\n",
       "      <td>(Glenglassaugh, 9.0)</td>\n",
       "      <td>(Glenglassaugh, 0.4729537233052701)</td>\n",
       "      <td>(Glenglassaugh, 0.6428571428571429)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Ardberg, 3.1622776601683795)</td>\n",
       "      <td>(Ardberg, 10.0)</td>\n",
       "      <td>(Tullibardine, 0.4737651884157824)</td>\n",
       "      <td>(Tullibardine, 0.6470588235294118)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Bruichladdich, 3.1622776601683795)</td>\n",
       "      <td>(Bruichladdich, 10.0)</td>\n",
       "      <td>(Glenmorangie, 0.49290744716289003)</td>\n",
       "      <td>(Glenmorangie, 0.6666666666666666)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Tullibardine, 3.3166247903554)</td>\n",
       "      <td>(Tullibardine, 11.0)</td>\n",
       "      <td>(Bruichladdich, 0.5000000000000001)</td>\n",
       "      <td>(Bruichladdich, 0.6666666666666666)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(Caperdonich, 3.4641016151377544)</td>\n",
       "      <td>(Caperdonich, 12.0)</td>\n",
       "      <td>(Ardberg, 0.5000000000000001)</td>\n",
       "      <td>(Ardberg, 0.6666666666666666)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(Deanston, 3.4641016151377544)</td>\n",
       "      <td>(Deanston, 12.0)</td>\n",
       "      <td>(Glen Deveron, 0.5101020514433645)</td>\n",
       "      <td>(Glen Deveron, 0.6842105263157895)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Bowmore, 3.4641016151377544)</td>\n",
       "      <td>(Bowmore, 12.0)</td>\n",
       "      <td>(Oban, 0.5398210066915778)</td>\n",
       "      <td>(Cragganmore, 0.7058823529411765)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(Glen Elgin, 3.4641016151377544)</td>\n",
       "      <td>(Glen Elgin, 12.0)</td>\n",
       "      <td>(Springbank, 0.5398210066915778)</td>\n",
       "      <td>(Glen Elgin, 0.7058823529411765)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(Benriach, 3.4641016151377544)</td>\n",
       "      <td>(Benriach, 12.0)</td>\n",
       "      <td>(Glenlossie, 0.5398210066915778)</td>\n",
       "      <td>(Oban, 0.7142857142857143)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             euclidean              cityblock  \\\n",
       "0                  (Bunnahabhain, 0.0)    (Bunnahabhain, 0.0)   \n",
       "1                 (Glenglassaugh, 3.0)   (Glenglassaugh, 9.0)   \n",
       "2        (Ardberg, 3.1622776601683795)        (Ardberg, 10.0)   \n",
       "3  (Bruichladdich, 3.1622776601683795)  (Bruichladdich, 10.0)   \n",
       "4      (Tullibardine, 3.3166247903554)   (Tullibardine, 11.0)   \n",
       "5    (Caperdonich, 3.4641016151377544)    (Caperdonich, 12.0)   \n",
       "6       (Deanston, 3.4641016151377544)       (Deanston, 12.0)   \n",
       "7        (Bowmore, 3.4641016151377544)        (Bowmore, 12.0)   \n",
       "8     (Glen Elgin, 3.4641016151377544)     (Glen Elgin, 12.0)   \n",
       "9       (Benriach, 3.4641016151377544)       (Benriach, 12.0)   \n",
       "\n",
       "                                cosine                              jaccard  \n",
       "0                  (Bunnahabhain, 0.0)                  (Bunnahabhain, 0.0)  \n",
       "1  (Glenglassaugh, 0.4729537233052701)  (Glenglassaugh, 0.6428571428571429)  \n",
       "2   (Tullibardine, 0.4737651884157824)   (Tullibardine, 0.6470588235294118)  \n",
       "3  (Glenmorangie, 0.49290744716289003)   (Glenmorangie, 0.6666666666666666)  \n",
       "4  (Bruichladdich, 0.5000000000000001)  (Bruichladdich, 0.6666666666666666)  \n",
       "5        (Ardberg, 0.5000000000000001)        (Ardberg, 0.6666666666666666)  \n",
       "6   (Glen Deveron, 0.5101020514433645)   (Glen Deveron, 0.6842105263157895)  \n",
       "7           (Oban, 0.5398210066915778)    (Cragganmore, 0.7058823529411765)  \n",
       "8     (Springbank, 0.5398210066915778)     (Glen Elgin, 0.7058823529411765)  \n",
       "9     (Glenlossie, 0.5398210066915778)           (Oban, 0.7142857142857143)  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's try more distances for the same whiskey\n",
    "whiskey_distance('Bunnahabhain', ['euclidean', 'cityblock', 'cosine', 'jaccard'], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show me your friends... and I'll make a prediction\n",
    "\n",
    "Is that the only thing we can do with similarity? No, not really.  We can also use similarity between objects (i.e., instances) to make _predictions_. To do so, we need to have labels on the _similar_ instances. How do you think we would do that?\n",
    "\n",
    "See the image below to guide your thinking. We want to classify the green instance. What should its class be?\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/850px-KnnClassification.svg.png\" width=\"30%\" />\n",
    "\n",
    "[Source: Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#/media/File:KnnClassification.svg)\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above approach is known as a **$k$-Nearest Neighbor** (**kNN**) classifier, because it finds the $k$ _most similar_ neighbors of the _instance_ that we want to predict.\n",
    "\n",
    "In the example of the Figure above, we used the Euclidean distance between two items (because... circles!). We collected the $k$ most similar instances in our datasets and combined their target variables to make our prediction.\n",
    "\n",
    "**Question:** How did we combine the target variables of the neighbors?\n",
    "> 1) look at what the **majority** is in the given distance ($k$); <br>\n",
    "2) look at the **weighted-distance** majority\n",
    "\n",
    "**Question:** How would we combine the target variable for a _regression_ problem?\n",
    "> \n",
    "\n",
    "**Question:** **kNN** is a _lazy_ classifier. Can you think why we call it that?\n",
    "> We're **not learning a model**. For building a model, we need the _entire dataset_. However, we only need $k$ amount of data for kNN.\n",
    "\n",
    "**Question:** What happens if we set $k$ to a _very large_ value, e.g., the total number of instances in our dataset?\n",
    "> (_Note: $k$ can be larger than the # of instances._)\n",
    "> In this case, the result will just be dependent on the majority of the total instances. Its significance will depend on the problem we're trying to solve.\n",
    "\n",
    "Let's work with our mailing campaign dataset which has a target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Income</th>\n",
       "      <th>Firstdate</th>\n",
       "      <th>Lastdate</th>\n",
       "      <th>Amount</th>\n",
       "      <th>rfaf2</th>\n",
       "      <th>glast</th>\n",
       "      <th>gavr</th>\n",
       "      <th>class</th>\n",
       "      <th>rfaa2_D</th>\n",
       "      <th>rfaa2_E</th>\n",
       "      <th>rfaa2_F</th>\n",
       "      <th>rfaa2_G</th>\n",
       "      <th>pepstrfl_0</th>\n",
       "      <th>pepstrfl_X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>9409</td>\n",
       "      <td>9509</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>9201</td>\n",
       "      <td>9602</td>\n",
       "      <td>0.16</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>20.55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>9510</td>\n",
       "      <td>9603</td>\n",
       "      <td>0.20</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>9409</td>\n",
       "      <td>9603</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>22.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>9310</td>\n",
       "      <td>9511</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>12.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Income  Firstdate  Lastdate  Amount  rfaf2  glast   gavr  class  rfaa2_D  \\\n",
       "0       3       9409      9509    0.06      1     50  30.00      0        0   \n",
       "1       2       9201      9602    0.16      4     20  20.55      1        0   \n",
       "2       0       9510      9603    0.20      4      5   8.75      0        0   \n",
       "3       6       9409      9603    0.13      2     25  22.50      0        0   \n",
       "4       0       9310      9511    0.10      1     25  12.50      0        0   \n",
       "\n",
       "   rfaa2_E  rfaa2_F  rfaa2_G  pepstrfl_0  pepstrfl_X  \n",
       "0        0        0        1           1           0  \n",
       "1        0        0        1           0           1  \n",
       "2        1        0        0           1           0  \n",
       "3        0        0        1           1           0  \n",
       "4        0        0        1           1           0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mail_data = pd.read_csv(\"data/mailing.csv\")\n",
    "mail_data = pd.get_dummies(mail_data)\n",
    "mail_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"data\" dataframe contains everything together.\n",
    "# Get the features separately from the class.\n",
    "X = mail_data.drop(['class'], axis=1)\n",
    "Y = mail_data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashtsoi/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Keep 75% of the data for training and 25% for testing. We have used this method before.\n",
    "X_mailing_train, X_mailing_test, Y_mailing_train, Y_mailing_test = train_test_split(X, Y, train_size=.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Great! We've read the data in, let's now learn a **kNN** classifier on our data.  The main parameter to consider here is the $k$ value, i.e., the number of neighbors that we want to base our decision making on.  For a full description of the parameters that the **kNN** classifier takes is available [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).\n",
    "\n",
    "For example, check what happens when the **weights** parameter isn't specified, versus the parameter being set to 'distance', i.e., **weights='distance'**.\n",
    "\n",
    "Also, check out the parameter **p** which controls the power of the Minkowski distance.\n",
    "\n",
    "The **metric** parameter controls which distance measure to consider between instances. More details about the values that the **metric** parameter can take can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's train a k-Nearest Neighbor classifier\n",
    "# model_mailing = KNeighborsClassifier(10)\n",
    "model_mailing = KNeighborsClassifier(10, weights='distance')   # Also try it with weights='distance'\n",
    "model_mailing.fit(X_mailing_train, Y_mailing_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(True) p</th>\n",
       "      <th>(True) n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[Predicted] Y</th>\n",
       "      <td>18</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[Predicted] N</th>\n",
       "      <td>2371</td>\n",
       "      <td>45438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               (True) p  (True) n\n",
       "[Predicted] Y        18       118\n",
       "[Predicted] N      2371     45438"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the trained model on the testing dataset and get the predicted classes\n",
    "predictions = model_mailing.predict(X_mailing_test)\n",
    "\n",
    "# Let's generate a confusion matrix\n",
    "conf_mtx = metrics.confusion_matrix(Y_mailing_test, predictions, labels=[1, 0])\n",
    "\n",
    "# Let's turn the confusion matrix into a DataFrame, to make it more presentable\n",
    "conf_mtx_df = pd.DataFrame(conf_mtx.T, columns=['(True) p', '(True) n'], index=['[Predicted] Y', '[Predicted] N'])\n",
    "conf_mtx_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it out with some different $k$-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with k = 1 is 0.905\n",
      "Accuracy with k = 10 is 0.948\n",
      "Accuracy with k = 50 is 0.949\n",
      "Accuracy with k = 100 is 0.949\n",
      "Accuracy with k = 1000 is 0.950\n",
      "Accuracy with k = 2000 is 0.950\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for k in [1, 10, 50, 100, 1000, 2000]:\n",
    "    model = KNeighborsClassifier(k, weights='distance')\n",
    "    model.fit(X_mailing_train, Y_mailing_train)\n",
    "    print (\"Accuracy with k = %d is %.3f\" % (k, metrics.accuracy_score(Y_mailing_test, model.predict(X_mailing_test))) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Class / Take-Home Assignment (Ungraded)\n",
    "\n",
    "Pick several non-negative integers, in ascending order, e.g., 1, 5, 10, 20, 50, or other values that you want. Each integer $d$ corresponds to the number of _features_ that you'll \"have\" available.\n",
    "\n",
    "Create two simple instances, both of which have the same number of features $d$. One instance will be all zeros and another one will be all ones. For example, if $d = 5$, then:\n",
    "$$ x_0 = (0, 0, 0, 0, 0) $$\n",
    "$$ x_1 = (1, 1, 1, 1, 1) $$\n",
    "\n",
    "To easily generate these instances, check the methods `np.zeros()` and `np.ones()` from the numpy library. More information here: \n",
    "* np.ones() - https://docs.scipy.org/doc/numpy/reference/generated/numpy.ones.html\n",
    "* np.zeros() - https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html\n",
    "\n",
    "\n",
    "Compute and print the Euclidean distance (L2) between the two instances, $x_0$ and $x_1$. Use the `distance.pdist()` method to compute easily compute the Euclidean distance between the instances.\n",
    "\n",
    "**Question 1:** What is happening with the distance as we add more features?\n",
    "\n",
    "**Question 2:** Repeat the same assignment, but generate instances with **randon** feature values. Check the `np.random.rand()` method (here: https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.rand.html#numpy.random.rand ) for how to do this conveniently. Since you are working with _random_ values, you should rerun the distances several times and get the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def eu_dists_over_d(d):\n",
    "    eu_dist = []\n",
    "    for n in range(1,d+1):\n",
    "        eu_dist.append(distance.pdist([np.zeros(n),np.ones(n)]))\n",
    "    \n",
    "    plt.plot(list(range(1,d+1)),eu_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the distances above, to see how they actually behave\n",
    "eu_dists_over_d(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question for you\n",
    "\n",
    "- Advantages of kNN ?\n",
    "\n",
    "> 1. Easier to explain/understand; <br>\n",
    "> 2. Scattered data; <br>\n",
    "> 3. Don't require a large amount of dataset; <br>\n",
    "> 4. Easy to implement.\n",
    "\n",
    "- Disadvantages of kNN ?\n",
    "\n",
    "> 1. Absolute distances; <br>\n",
    "> 2. hi.\n",
    "\n",
    "- Also check the previous questions in this notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
