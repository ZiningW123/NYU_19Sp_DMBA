{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Extra Credit Assignment (5 points)\n",
    "**Deadline:** Friday, April 5, 2019 11:59:59pm. I will create an assignment on Classes, where you can submit your results.\n",
    "\n",
    "As part of this assignment, there is an attached zip file containing two files: `reviews/reviews_train.tsv` and `reviews/reviews_test.tsv`. Each file contains a number of lines. Each line is separated by a **tab value**. The first part of the line is a piece of text (a \"review\"). The second part of the line is an integer value, from 1 to 5, corresponding to the star rating that was given alongside the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 10, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, you will need to upload these files to your JupyterHub Notebook server. You can upload files by simple drag 'n' drop, or by using the \"Upload\" button.\n",
    "\n",
    "* You will use the `reviews_train.tsv` file to train various classification models (see below). You will then use the trained model(s) on the `reviews_test.tsv` file to test their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes!\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "something = '4.0'\n",
    "if float(something) == 4:\n",
    "    print('yes!')\n",
    "    print(int(float(something)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file and clean up data\n",
    "with open('reviews_train.tsv', encoding=\"utf8\") as tr:\n",
    "    train_rawdata = [x_tr.strip().split(\"\\t\") for x_tr in tr.readlines()] \n",
    "    train_data_cleaned = []\n",
    "    for data in train_rawdata:\n",
    "        tr_cleaned = {'text': data[0], 'label': int(float(data[1]))}\n",
    "        train_data_cleaned.append(tr_cleaned)\n",
    "        \n",
    "train_data = pd.DataFrame(train_data_cleaned)\n",
    "\n",
    "with open('reviews_test.tsv', encoding=\"utf8\") as te:\n",
    "    test_rawdata = [x_te.strip().split(\"\\t\") for x_te in te.readlines()] \n",
    "    test_data_cleaned = []\n",
    "    for data in test_rawdata:\n",
    "        te_cleaned = {'text': data[0], 'label': int(float(data[1]))}\n",
    "        test_data_cleaned.append(te_cleaned)\n",
    "        \n",
    "test_data = pd.DataFrame(test_data_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ======= TRAIN DATA ======= \n",
      "    label                                               text\n",
      "0      4  Proximity to waterfront and downtown Seattle. ...\n",
      "1      4  Clean, shopping near by, very pleasant staff, ...\n",
      "2      5  Everything about our stay was great. Traveling...\n",
      " \n",
      " ======= TEST DATA ======= \n",
      "    label                                               text\n",
      "0      5  This was our first time visiting the city of N...\n",
      "1      5  Excellent service, great size rooms and great ...\n",
      "2      5  Great new hotel with amazing access to shoppin...\n"
     ]
    }
   ],
   "source": [
    "print(' ======= TRAIN DATA ======= \\n', train_data.head(3))\n",
    "print(' \\n ======= TEST DATA ======= \\n', test_data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, you will train your classifiers on the contents of the `reviews_train.tsv` file (text and label), as they are, meaning you will be working with 5 different classes (the different star ratings that show up in the file). You will train **4 different classifiers**. \n",
    "\n",
    "Each of those classifiers will be tested on the contents of the `reviews_test.tsv` file. For the evaluation, you will be using CRCs (Cumulative Response Curves). You will have to create **2 CRCs**.\n",
    "\n",
    "Each CRC will demonstrate (in the same plot) the performance of each of the 4 classifiers on the `reviews_test.tsv`, when focusing on a _specific star rating_. You get to pick the star ratings to test. For example, if you pick \"rating = 2\", then the CRC will consider as correct classifications on the `reviews_test.tsv` the ratings that are equal to 2, and will consider as misclassification anything else.\n",
    "\n",
    "Which two star ratings to show (one per CRC) is up to you. However, one rating must be below 3 and the other greater than or equal to 3. For example, you can pick to plot the pair (\"Rating = 2\", \"Rating = 5\"). You cannot pick both plots to be < 3 or both of them to be >= 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I -- 4 Classifiers & CRC\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision-tree model\n",
    "model_tree = DecisionTreeClassifier(criterion='entropy', max_depth=15)\n",
    "\n",
    "# Logistical Regression model\n",
    "model_logregr = LogisticRegression(C=100)\n",
    "\n",
    "# The SVM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier #4 -- Naive Bayes (NB) \n",
    "_[using Bernoulli Naive Bayes (BNB)]_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total instances of the vocabs in all text\n",
    "binary_vectorizer_tr = CountVectorizer(binary=True)\n",
    "binary_vectorizer_tr.fit(train_data['text'])\n",
    "\n",
    "vocab_ls_tr = list(zip( binary_vectorizer_tr.vocabulary_.keys(), binary_vectorizer_tr.vocabulary_.values()) )\n",
    "vocab_ls_tr[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "\n",
    "# Naive Bayes has an alpha parameter, which operates exactly like the lambda parameter for Logistic Regression\n",
    "model = BernoulliNB()\n",
    "model.fit(X_train_counts, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"AUC on the count TRAIN data = %.3f\" % metrics.roc_auc_score(Y_train, model.predict(X_train_counts)))\n",
    "print (\"AUC on the count TRAIN data = %.3f\" % metrics.roc_auc_score(Y_train, model.predict_proba(X_train_counts)[:, 1]))\n",
    "\n",
    "print (\"AUC on the count TEST data = %.3f\" % metrics.roc_auc_score(Y_test, model.predict(X_test_counts)))\n",
    "print (\"AUC on the count TEST data = %.3f\" % metrics.roc_auc_score(Y_test, model.predict_proba(X_test_counts)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can reuse the same classifiers in the two CRCs.\n",
    "\n",
    "\n",
    "**Note 1:** When training your classifiers, you will have to train them on the dataset with the 5 different classes together. When testing, you will be focusing on a specific rating, one for each CRC that you generate.\n",
    "\n",
    "\n",
    "**Note 2:** As you are dealing with text, you may consider the text representation transformation as a different \"classifier\". That is, you can consider the the `CountVectorizer` with binary feature values and the `CountVectorizer` with actual counts as two different \"classifiers\", even though you use, e.g., `LogisticRegression` for both of them. On the other hand, changing the complexity parameters of a classifier _alone_ is not considered a separate classifier for the purposes of this assignment.\n",
    "\n",
    "\n",
    "Make sure that in your plots you specify which model corresponds to which line. Give a brief description of the results and say which model you would pick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II -- 2 CRCs\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a method that trains and returns the CRC of a clasifier\n",
    "\n",
    "def train_and_compute_crc( model, x_train, y_train, x_test, y_test ):\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Let's get the probabilities. FOCUS ON THE POSITIVE CLASS\n",
    "    probabilities = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "    # Create a dataframe that we can conveniently manipulate\n",
    "    model_df = pd.DataFrame(list(zip(probabilities, y_test)), columns=[\"PROBABILITY\", \"TRUE_CLASS\"])\n",
    "\n",
    "    # Sort the dataframe rows by the PROBABILITY\n",
    "    model_df_sorted = model_df.sort_values(by=['PROBABILITY'], ascending=False)\n",
    "\n",
    "    # Compute the CUMULATIVE correct responses up until the\n",
    "    return model_df_sorted[\"TRUE_CLASS\"].cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CRC #1 -- Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CRC #2 -- Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of CRC for all four classifiers\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the above results, together\n",
    "plt.plot(range(0, len(dec_tree_crc)), dec_tree_crc, label=\"Decision Tree\")\n",
    "plt.plot(range(0, len(logReg_crc)), logReg_crc, label=\"Logistic Regression\")\n",
    "plt.plot([0,len(logReg_crc)], [0,max(logReg_crc)], 'k--', label=\"Random\")\n",
    "plt.xlabel(\"Number of test instances targeted (decreasing score)\")\n",
    "plt.ylabel(\"Number of positives targeted\")\n",
    "plt.title(\"Cumulative response curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful note for the data\n",
    "\n",
    "When dealing with text, it is often the case that we must work with Unicode characters. \n",
    "\n",
    "If you encounter the error: \"ValueError: np.nan is an invalid document, expected byte or unicode string.\". You are welcome to ask questions for the EC on the forums.\n",
    "\n",
    "check the following url: https://stackoverflow.com/questions/39303912/tfidfvectorizer-in-scikit-learn-valueerror-np-nan-is-an-invalid-document to help you resolve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
