{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining for Business Analytics\n",
    "## From correlation to supervised segmentation and tree-structured models\n",
    "\n",
    "Spring 2019 - Prof. George Valkanas\n",
    "\n",
    "Material based on content courtesy of Prof. Foster Provost\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap and Motivation\n",
    "\n",
    "The company we work for is running an ad-campaign for a new service that we want to offer. Our company has done similar campaigns in the past and has historical information about people we have previously reached out. For each such person, we know how they responded (\"**Yes**\", \"**No**\") plus certain characteristics that each individual has.\n",
    "\n",
    "Reaching out to an individual incurs a certain cost. So, we would like to offer this new service to individuals who will respond positively.\n",
    "\n",
    "\n",
    "**Questions**\n",
    "* What type of (data mining) problem are we discussing?\n",
    "* What is the target variable in our setting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access to Historical Data\n",
    "A sample of the people we have reached out to _in the past_ is shown in the image below, on the left hand side. Some of those people have responded positively (\"Yes\"), whereas others have responded negatively (\"No\").\n",
    "\n",
    "Despite the easier visual presentation, the information on the left hand side image is not nicely _structured_. For data mining tasks, we (typically) want our data / information organized as shown on the right hand side image.\n",
    "\n",
    "Of course, this is only an example, and we can have many more characteristics (read: attributes / features) for every individual. Such attributes may be:\n",
    "* Marital status\n",
    "* Number of kids\n",
    "* Number of accepted invites in the past\n",
    "* Number of rejected invites in the past\n",
    "* Currently active user (with the service)\n",
    "* Current plan cost (or average plan cost, if multiple ones)\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"border: 0px\">\n",
    "    <tr style=\"border: 0px\">\n",
    "        <td style=\"width: 35%; text-align: justify\">\n",
    "            <img width=\"100%\" src=\"images/population.png\"/> <br/>\n",
    "            <span style=\"display: block; text-align: center\">\n",
    "                <b>Sample population of previous respondents</b>\n",
    "            </span>\n",
    "        </td>\n",
    "        <td style=\"border: 0px; width: 5%\"></td>\n",
    "        <td style=\"display: block; text-align: justify\">\n",
    "            <img src=\"images/dataset.png\" height=75% width=75% /> <br/>\n",
    "            <span style=\"display: block; text-align: center\">\n",
    "                <b>A structured dataset for a data mining task</b>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<br/>\n",
    "\n",
    "The business problem that we mentioned above maps to the following **data mining question:** _Given an individual's characteristics (age, salary, employment status, etc), how will they respond to our campaign?_\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's work with some data\n",
    "\n",
    "For now, we will simply generate some data to play with. Though a real dataset would be more meaningful from an application standpoint, we would have to spend a lot of time to understand it, and even then, we would still be using only a portion of it to explain the core concepts below.\n",
    "\n",
    "We will also start our scenario by working with two (2) dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to need a lot of Python **packages**, so let's start by importing all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we will be using\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from IPython.display import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also going to do a lot of repetitive stuff, so let's predefine some useful **FUNCTIONS**:\n",
    "\n",
    "Remember the basic aspects of a function:\n",
    "\n",
    "* Input -> Parameters\n",
    "* Actions\n",
    "* Output -> Return\n",
    "\n",
    "(We might have no _return_ which is basically just an action usually known as procedure. Also, we will see in other classes that these functions can also be taken from a **SCRIPT !!!**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A function that picks a color for an instance, depending on its target variable\n",
    "# We use 0 for \"No\" and \"1\" for yes.\n",
    "# The function returns a list of items, one item for each instance (in the order given)\n",
    "def Color_Data_Points(target):\n",
    "    color = [\"red\" if t == 0 else \"blue\" for t in target]\n",
    "    return color\n",
    "\n",
    "\n",
    "# A function to plot the data in a scatter plot\n",
    "# data: The data we want to visualize\n",
    "# v1: The name used to access the X-axis variable information in the data parameter\n",
    "# v2: The name used to access the Y-axis variable information in the data parameter\n",
    "# tv: The name used to access the target variable information in the data parameter\n",
    "def Plot_Data(data, v1, v2, tv):\n",
    "\n",
    "    # Make the plot square\n",
    "    plt.rcParams['figure.figsize'] = [12.0, 8.0]\n",
    "    \n",
    "    # Color\n",
    "    color = Color_Data_Points(data[tv])\n",
    "    \n",
    "    # Plot and label\n",
    "    plt.scatter(data[v1], data[v2], c=color, s=50)\n",
    "    plt.xlabel(v1)\n",
    "    plt.ylabel(v2)\n",
    "    plt.xlim([min(data[v1]) , max(data[v1]) ])\n",
    "    plt.ylim([min(data[v2]) , max(data[v2]) ])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need some data, so let's create a dataset consisting of **500** people (rows) with **3** different variables (columns): `[\"name\", \"age\", \"years_customer\"]` \n",
    "\n",
    "The **target** of our prediction will be whether or not a person successfully responded to a previous ad that we ran. We will call it \"response\" ( **binary** -> 0/1 values )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the randomness\n",
    "np.random.seed(36)\n",
    "\n",
    "# Number of users, i.e. number of instances in our dataset\n",
    "n_users = 500\n",
    "\n",
    "# Features that we know about each user. The attributes below are for illustration purposes only!\n",
    "variable_names = [\"name\", \"age\", \"years_customer\"]\n",
    "variables_keep = [\"years_customer\", \"age\"]\n",
    "target_name = \"response\"\n",
    "\n",
    "# Generate data with the \"datasets\" function from SKLEARN (package)\n",
    "# This function returns two variables: predictors and target\n",
    "\n",
    "predictors, target = datasets.make_classification(n_features=3, n_redundant=0, \n",
    "                                                  n_informative=2, n_clusters_per_class=2,\n",
    "                                                  n_samples=n_users)\n",
    "\n",
    "# We will write this data in a dataframe (pandas package)\n",
    "\n",
    "data = pd.DataFrame(predictors, columns=variable_names)\n",
    "\n",
    "# We want to take each column of the dataframe to change the values \n",
    "\n",
    "data['age'] = data['age'] * 10 + 50\n",
    "data['years_customer'] = (data['years_customer'] + 6)/2\n",
    "data[target_name] = target\n",
    "\n",
    "# Our variables (features) will be stored in one variable called X\n",
    "X = data[variables_keep]\n",
    "\n",
    "# Our target will be stored in one variable called Y\n",
    "Y = data[target_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can (and should) take a look at the first few rows/records of our data to see what we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first 5 values of our data\n",
    "pd.concat([X, Y], axis=1).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualize our data first\n",
    "Before diving into the actual modeling, it is (generally) **great** practice to visualize the data that we have. Because we are dealing with _two-dimensional (2D) numerical data_ we can visualize our information in a _scatter plot_. Let's do that right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[7,6])\n",
    "Plot_Data(data, \"years_customer\", \"age\", \"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-Step Approach\n",
    "\n",
    "Before we dive into the technical solution of the problem, let's first state what we have and what we want to do. We have a 2D dataset, with two different classes. We are looking for a structured, repeatable and concise approach (i.e. **a model**) to distinguish between the two classes.\n",
    "\n",
    "For a number of reasons, including simplicity, let's say that all we can do is draw **straight lines**. In fact, let's make things even simpler and say that we can only draw straight lines that are _parallel to the axes_ (either the X or the Y axis).\n",
    "\n",
    "**Question 1:** Which axis (the \"X\" or the \"Y\") looks like a better candidate ?\n",
    "\n",
    "**Question 2:** Where should we place the straight line in the above dataset, in order to _best_ separate the data points of the two classes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One feature and splits\n",
    "Let's take a look at one of our features -- **`\"age\"`**. Is this feature useful? \n",
    "\n",
    "- Let's look at a scatter plot the possible values of `\"age\"` and color code our target variable, `\"response\"`. Red dots mean unsuccessful people and blue dots mean successful people. \n",
    "- The horizontal value is the number of pets they have. The vertical position here doesn't mean anything (everyone has a one).\n",
    "\n",
    "**Question:** By loooking at the scatter plot below, and only that information, does this look like a good variable to use to separate between the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15.0, 2.0]\n",
    "\n",
    "color = color = Color_Data_Points(data[\"response\"])\n",
    "plt.scatter(X['age'], Y, c=color, s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Produce the scatter plot for the variable **`\"years_customer\"`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy ($H$) and Information Gain ($IG$)\n",
    "***\n",
    "\n",
    "Our earlier approach was guided visually. Though this may work well for us humans, it does not work for computers as nicely.\n",
    "\n",
    "To achieve the same result mathematically, we will use **Entropy** ($H$). Entropy is a measure from thermodynamics that tells us how (dis)ordered a system is. We will use entropy to quantify the _homogeneity_ (or `purity`) of our data.\n",
    "\n",
    "Mathematically, the entropy of a set with respect to a property of interest - in our case, the _target variable_, is given by the following quantity:\n",
    "\n",
    "$ entropy = - p_1 \\times \\log(p_1) - p_2 \\times \\log(p_2) - \\dots $\n",
    "\n",
    "<img src=\"images/entropy.png\" height=50% width=50%>\n",
    "\n",
    "\n",
    "For our sample dataset, the entropy is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_class_count = Y[Y == 1].count()\n",
    "positive_fraction = positive_class_count / Y.count()\n",
    "negative_fraction = (Y.count() - positive_class_count) / Y.count()\n",
    "entropy = -positive_fraction * math.log(positive_fraction, 2) - negative_fraction * math.log(negative_fraction, 2)\n",
    "\n",
    "print(\"Total data points: \", Y.count())\n",
    "print(\"Positive data points: \", positive_class_count)\n",
    "print(\"Negative data points: \", (Y.count() - positive_class_count))\n",
    "print(\"Entropy is\", entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information gain** ($IG$) builds on entropy and helps us determine which feature(s) is the most informative.\n",
    "\n",
    "There are two steps in this process: \n",
    "1. We must pick a _feature_ to split\n",
    "2. We must pick a _value_ of that feature to split on\n",
    "\n",
    "Now, before splitting on a value, we have the entropy of the entire set, which we typically call the `parent` set when discussing information gain. After splitting, we get the `children` sets, and we can compute the entropy _for each_ of those sets. The information gain amount is given by the difference between the `parent entropy` and the sum of the `children entropies`.\n",
    "\n",
    "Formally:\n",
    "\n",
    "$IG(parent, children) = entropy(parent) - [p(c_1)\\times entropy(c_1) + p(c_2)\\times entropy(c_2) + \\dots]$\n",
    "\n",
    "The values $c_1$, $c_2$ and so on refer to the different children that we get by splitting on\n",
    "\n",
    "Let's see an example with the split and information gain where we use the numeric `Balance` attribute (left hand side) and the categorical `Residence` attribute (right hand side).\n",
    "_Note:_ these are new attributes, that we haven't mentioned above. Again, this is for illustration purposes.\n",
    "\n",
    "<table style=\"border: 0px\">\n",
    "    <tr style=\"border: 0px\">\n",
    "        <td style=\"border: 0px; width: 45%; text-align: justify\">\n",
    "            <img src=\"images/dsfb_0304.png\" height=80% width=80%> <br/>\n",
    "            <b>Figure 3-4. Splitting the \"response\" sample into two segments, by splitting the Balance attribute (account balance) at 50K.</b>\n",
    "        </td>\n",
    "        <td style=\"border: 0px; width: 5%\"></td>\n",
    "        <td style=\"border: 0px; width: 45%; text-align: justify\">\n",
    "            <img src=\"images/dsfb_0305.png\" height=100% width=100%>\n",
    "            <b>Figure 3-5. A classification tree split on the three-values Residence attribute.</b>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**Question:** Can you spot a difference between the two attributes above?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions to get the entropy and IG\n",
    "Entropy and information gain are both relatively easy to calculate. As you can probably tell, we also use them quite repeatedy during the splitting process, so it makes sense to have them as methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def entropy(target):\n",
    "    # Get the number of instances\n",
    "    n = len(target)\n",
    "    # Count how frequently each unique target value occurs using the numpy function \n",
    "    counts = np.bincount(target).astype(float)\n",
    "    # Initialize entropy\n",
    "    entropy = 0\n",
    "    \n",
    "    # Otherwise, for each possible value, update entropy; use zero for 0 log 0\n",
    "    for count in counts:\n",
    "        if count == 0:\n",
    "            entropy += 0\n",
    "        else:\n",
    "            entropy += math.log(count/n, 2) * count/n\n",
    "    # Return entropy\n",
    "    return -1 * entropy\n",
    "\n",
    "def information_gain(feature, threshold, target):\n",
    "    # Dealing with numpy arrays makes this slightly easier\n",
    "    target = np.array(target)\n",
    "    feature = np.array(feature)\n",
    "    # Cut the feature vector on the threshold\n",
    "    feature = (feature < threshold)\n",
    "    # Initialize information gain with the parent entropy\n",
    "    ig = entropy(target)\n",
    "    # For both sides of the threshold, update information gain\n",
    "    for level, count in zip([0, 1], np.bincount(feature).astype(float)):\n",
    "        ig -= count/len(feature) * entropy(target[feature == level])\n",
    "    # Return information gain\n",
    "    return ig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Let's go back to our toy example. Let's take a look again at our toy data, focusing on the `years_customer` feature alone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15.0, 2.0]\n",
    "\n",
    "color = Color_Data_Points(data[\"response\"])\n",
    "plt.scatter(X['years_customer'], Y, c=color, s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already said that `years_customer` looks like a good feature to split on. In fact, we said that `years_customer = 3` looks like a good threshold to use!\n",
    "\n",
    "Let's see what we get with that `(feature, threshold)` combination in practice:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_feature = \"years_customer\"  # The feature we are splitting\n",
    "threshold = 3 # The threshold that we decided is a good one\n",
    "print (\"IG = %.4f with thresholding of %.2f.\" % (information_gain(X[split_feature], threshold, np.array(Y)), threshold))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be more precise, we can iterate through all values and find the best split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def best_threshold():\n",
    "    maximum_ig = 0\n",
    "    maximum_threshold = 0\n",
    "\n",
    "    split_on_feat = 'years_customer'\n",
    "\n",
    "    for threshold in X[split_on_feat]:\n",
    "        ig = information_gain(X[split_on_feat], threshold, np.array(Y))\n",
    "        if ig > maximum_ig:\n",
    "            maximum_ig = ig\n",
    "            maximum_threshold = threshold\n",
    "\n",
    "    return \"The maximum IG = %.3f and it occured by splitting on %.4f.\" % (maximum_ig, maximum_threshold)\n",
    "\n",
    "print ( best_threshold() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features and splits with the sklearn package !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can do this with just sklearn! Now, we will be using all the variables in X, not only number of pets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that gives a visual representation of the decision tree\n",
    "\n",
    "def Decision_Tree_Image(decision_tree, feature_names, class_names, name=\"tree\",proportion=True):\n",
    "    \n",
    "    # Export our decision tree to graphviz format\n",
    "    dot_file = tree.export_graphviz(decision_tree, out_file='images/' + name + '.dot', \n",
    "                                    feature_names=feature_names, class_names=class_names,proportion=proportion)\n",
    "        \n",
    "    # Call graphviz to make an image file from our decision tree\n",
    "    os.system(\"dot -Tpng images/\" + name + \".dot -o images/\" + name + \".png\")\n",
    "    # to get this part to actually work, you may need to open a terminal window in Jupyter and run the following command \"sudo apt install graphviz\"\n",
    "    \n",
    "    # Return the .png image so we can see it\n",
    "    return Image(filename='images/' + name + '.png')\n",
    "\n",
    "# A function that creates the surface of a decision tree\n",
    "\n",
    "def Decision_Surface(data, target, model):\n",
    "    # Get bounds\n",
    "    x_min, x_max = data[data.columns[0]].min(), data[data.columns[0]].max()\n",
    "    y_min, y_max = data[data.columns[1]].min(), data[data.columns[1]].max()\n",
    "\n",
    "    # Create a mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max,0.01), np.arange(y_min, y_max,0.01))\n",
    "    meshed_data = pd.DataFrame(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    plt.figure(figsize=[12,7])\n",
    "    Z = model.predict(meshed_data).reshape(xx.shape)\n",
    "\n",
    "    plt.title(\"Decision surface\")    \n",
    "    plt.ylabel(\"age\")\n",
    "    plt.xlabel(\"years_customer\")\n",
    "\n",
    "    color = Color_Data_Points(target)\n",
    "    cs = plt.contourf(xx, yy, Z, levels=[-1,0,1], colors=['#ff6666', '#66b2ff'] )\n",
    "    plt.scatter(data[data.columns[0]], data[data.columns[1]], color=color, edgecolor='black' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the model (tree)\n",
    "my_tree = DecisionTreeClassifier(max_depth=10,criterion=\"entropy\")   # Look at those 2 arguments !!! \n",
    "\n",
    "# Let's tell the model what is the data\n",
    "my_tree.fit(X, Y)\n",
    "\n",
    "#Let's print an image with the results\n",
    "Decision_Tree_Image(my_tree, X.columns, class_names =['fail','success'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `\"age\"`and `\"years_customer\"`, including the **DECISION SURFACE!!**\n",
    "\n",
    "More details for this graph: [sklearn decision surface](http://scikit-learn.org/stable/auto_examples/tree/plot_iris.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decision_Surface(X,Y,my_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ( \"Accuracy = %.3f\" % (metrics.accuracy_score(my_tree.predict(X), Y)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
