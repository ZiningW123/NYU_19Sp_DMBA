{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Name: Ashley Tsoi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your NetID: ast418\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Part A\n",
    "\n",
    "### <div style=\"color: red\">Read Carefully Before Proceeding</div>\n",
    "\n",
    "If you are having issues with running this code because of missing libraries, check the material that we've done in class for installation instructions. This code uses what we have already seen, so if you've been able to execute the code of the Notebooks we've seen in class, you will be fine here as well.\n",
    "\n",
    "\n",
    "You need to answer all questions. Make sure that you answer both **technical** (code-related) and **non-technical** (conceptual) parts of this homework. A lot of code is already available for you, and you can build on that. You are free to use code from our notebooks in class.  All visualizations must be generated by your code, programmatically, unless explicitly mentioned otherwise by a question.\n",
    "\n",
    "\n",
    "Once you're done, download the notebook via `File` -> `Download as` -> `Notebook`, which will fetch a file with an \".ipynb\" extension. Include this file in your submission, as a separate document -- **not** in the word / pdf submission itself. In case you use additional code stored in another directory, make sure to submit that as well.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bunch of packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import math\n",
    "import pyproj as proj\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "plt.rcParams['figure.figsize'] = 10, 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Basic Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Reconstructing a Confusion Matrix\n",
    "\n",
    "Using the following values on some evaluation measures:\n",
    "\n",
    "* \\# Instances = 1000\n",
    "* Accuracy = 70%\n",
    "* Precision = 62.5%\n",
    "* Recall = 71.43%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** There are several ways to present the confusion matrix. Either as a 2x2 square or with the individual values. Feel free to follow an approach that you prefer, but make sure that the information is legible and clearly represents what you computed. You may upload an image with the confusion matrix shown as part of your submission if you prefer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A) Reconstruct the corresponding confusion matrix.**\n",
    "\n",
    "> **accuracy** = $\\frac{TP+TN}{N}$\n",
    "> **precision** = $\\frac{TP}{TP+FP}$\n",
    "> **recall** = $\\frac{TP}{TP+FN}$\n",
    "\n",
    "> $ \\frac{1}{precision} + \\frac{1}{recall} = \\frac{1}{6.25\\%} + \\frac{1}{71.43\\%} \\approx 3 $ \n",
    "<br><br>\n",
    "> $ \\frac{2TP + FP + FN}{TP} = 2 + \\frac{FP + FN}{TP} \\approx 3 $\n",
    "<br><br>\n",
    "> $ \\frac{FP + FN}{TP} \\approx 1 $\n",
    "<br><br>\n",
    "> $ TP \\approx FP + FN = 1000 - 70\\%\\times1000 = 300 $ <br>\n",
    "--> $TN = 70\\%\\times1000 - TP = 400$\n",
    "<br><br>\n",
    "> $ 62.5\\% = \\frac{TP}{TP+FP} = \\frac{300}{300+FP}$ <br>\n",
    "--> $FP = 180$ --> $FN = 300 - FP = 120$\n",
    "<br>\n",
    "\n",
    "> |      _        | Predicted: YES | Predicted: NO | TOTAL       |\n",
    "| ------------- | -------------- | ------------- | ----------- |\n",
    "|**Actual: YES**| `TP = 300    ` | `FN = 120   ` | TP+FN = 420 |\n",
    "|**Actual: NO** | `FP = 180    ` | `TN = 400   ` | FP+TN = 580 |\n",
    "|**TOTAL**      | TP+FP = 480    | FN+TN = 520   | N = 1000    |\n",
    "\n",
    "<br>\n",
    "\n",
    "**B) Report the TPR value of that confusion matrix.** <br><br>\n",
    "> $ TPR = \\frac{TP}{TP+FN} = \\frac{300}{420} \\approx 71.43\\% $<br>\n",
    "\n",
    "\n",
    "**C) Report the FPR value of that confusion matrix.** <br><br>\n",
    "> $ FPR = \\frac{FP}{FP+TN} = \\frac{180}{580} \\approx 31.03\\% $<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - \"Reading\" a Cumulative Response Curve\n",
    "\n",
    "As part of a recent project that you did for your business, you wanted to present the **savings that you'd get by targeting a subset of the available population**. The results are presented to the business side. After training and evaluating your classifier, you presented your results in the form of a **Cumulative Response Curve (CRC plot)**. The CRC that you got from your classifier is shown below.\n",
    "\n",
    "<img src=\"imgs/crc.png\" height=\"40%\" width=\"40%\" />\n",
    "\n",
    "With that CRC in mind and your general knowledge of evaluation curves, answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Why pick a CRC ?\n",
    "\n",
    "**In 2-3 sentences (max), discuss some reasons why a CRC is a better option over other evaluation curves (e.g., ROC) and measures (e.g., AUC, precision, etc), considering the original goal (see above).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Curves like ROC are common for visualization of the **classification performance**, but are not as **intuitive** for business stakeholders to understand the potential result of the implementation. While CRC and its measures such as the AUC and precision are good single-number measures that describes the performance, the CRC is easier for businesses to connect the graph's information to the actual implications of their business problem (how much savings they can potentially save from better classification in this case).\n",
    "\n",
    "> In short, CRC is more straight-forward and intuitive (esp. for outsiders), while some of the other curves are more comprehensive for understanding the model performance (for geeks/model-makers).\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2: \"Reading\" the CRC\n",
    "\n",
    "Imagine that you are presenting your findings and you want to illustrate the savings that you'd achieve by using your classifier. Using the previous CRC, give **3 examples** that will help your audience understand the trade-off between the savings and the performance that you'd (roughly) get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. **Customer churn** --\n",
    "  If there are 100 potential customers that we can target, we can \n",
    "\n",
    "> 2. **Advertisement targeting** --\n",
    "  \n",
    "\n",
    "> 3. **Recruiting** -- \n",
    "  If\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Data Driven Recruiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 - Defining the problem\n",
    "\n",
    "The recruiting paradigm has changed over the years, with several companies priding themselves in following data-driven hiring practices and processes. That is, they rely heavily on _data_ to decide whether someone should be hired or not, among other things.\n",
    "\n",
    "You're helping build such a system and you're treating it as a _classification_ problem. An _instance_ is a candidate considered for a particular role. \n",
    "\n",
    "* What is the **target variable** of your problem?\n",
    "\n",
    "Think carefully before you answer.  _Hiring_ is an action that we take for _something else_ that we are trying to capture / predict.  Consider the analogy with the marketing scenario that we've seen in class: we invite people (action) because we predicted they will _donate money_ (target variable). Similarly, we _hire_ people (action) because .... . The target variable has to be something **measurable / quantifiable**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer here_\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 - Recruiting False Positives & False Negatives\n",
    "\n",
    "We've seen that a confusion matrix has four corners, for a binary classification problem: $TP$, $FP$, $FN$ and $TN$. For some companies, the following motto has been heard in relation to their hiring practices:\n",
    "\n",
    "\"_A False Positive (FP) is worse / costlier than a False Negative (FN)_\"\n",
    "\n",
    "Very briefly answer the following:\n",
    "\n",
    "* What is a False Positive in this case?\n",
    "* What is a False Negative in this case?\n",
    "* What do you think the above expression means?\n",
    "* What _performance measure_ (i.e., accuracy, precision, recall, F1 measure, etc) do they try to optimize / improve following the logic of that expression? Explain your thinking.\n",
    "\n",
    "\n",
    "Be careful: the question is **not** asking for a general definition of FP and FN. It is asking for what they mean in this context. As a hint, your answers should be something along the lines of \"A FP refers to a candidate ....\".\n",
    "\n",
    "Your answer must also be aligned with your previous definition of a target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer here_\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Dispatching Emergency Vehicles\n",
    "\n",
    "Data Mining / Data Science is so pervasive that there is the consideration (in fact, implementation) of using such techniques to decide whether an emergency vehicle (police car, ambulance, etc) should be dispatched with respect to a 911 call.\n",
    "\n",
    "An overly simplistic way to approach this problem is via **a classification framework that results in an \"dispatch / don't dispatch car\" action**.\n",
    "\n",
    "**What is the proper way of evaluating such an approach**, i.e. which evaluation measure would you use? Accuracy? AUC? An evaluation curve? Something else? Explain your thinking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer here_\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
