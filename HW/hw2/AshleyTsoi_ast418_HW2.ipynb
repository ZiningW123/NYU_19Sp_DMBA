{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><div style=\"color:red\">COMMENTS:</div></b>\n",
    "\n",
    "Check for comments in the notebook. If you have issues with the coding assignments, or if you get stuck, always feel free to reach out to me to discuss them, so that you can resolve them.\n",
    "\n",
    "Also, if you receive help with the assignment, you are expected to report it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Section (HW2)\n",
    "\n",
    "### Read Carefully Before Proceeding\n",
    "\n",
    "If you are having issues with running this code because of missing libraries, check the material that we've done in class for installation instructions. This code uses what we have already seen, so if you've been able to execute the code of the Notebooks we've seen in class, you will be fine here as well.\n",
    "\n",
    "\n",
    "You need to answer all questions. Make sure that you answer both **technical** (code-related) and **non-technical** (conceptual) parts of this homework. A lot of code is already available for you, and you can build on that. You are free to use code from our notebooks in class.  All visualizations must be generated by your code, programmatically.\n",
    "\n",
    "\n",
    "Once you're done, download the notebook via `File` -> `Download as` -> `Notebook`, which will fetch a file with an \".ipynb\" extension. Include this file in your submission, as a separate document -- **not** in the word / pdf submission itself. In case you use additional code stored in another directory, make sure to submit that as well.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Customer Churn\n",
    "\n",
    "In this section, you will demonstrate your technical and other Data Science skills, when applied to the Customer Churn problem. MTC has given you access to a small subset of their data with information they have collected according to your specifications.\n",
    "\n",
    "As such, they have provided two different datasets: a **training** ('churn_train.csv') and a **testing** one ('churn_test.csv'). You do not know how they picked which instances to place in the two files. Both datasets contain the same _features_ and they both have a target variable: whether the customer left or not. The target variable is the last \"column\" of the csv file(s), named `LEAVE`.\n",
    "\n",
    "\n",
    "These two datasets are located within the `data/` folder. These are csv files, which you can open and have a look if interested. Nevertheless, the goal is to use your technical skills to address the problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following good practices, import everything we'll need first. If you need to import more libraries, you can do so here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read the two files. Pandas have a very convenient way to read CSV files.\n",
    "train_orig = pd.read_csv('data/churn_train.csv')  # Start with the training dataset. \n",
    "test_orig = pd.read_csv('data/churn_test.csv')  # Proceed with the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COLLEGE</th>\n",
       "      <th>INCOME</th>\n",
       "      <th>OVERAGE</th>\n",
       "      <th>LEFTOVER</th>\n",
       "      <th>HOUSE</th>\n",
       "      <th>HANDSET_PRICE</th>\n",
       "      <th>OVER_15MINS_CALLS_PER_MONTH</th>\n",
       "      <th>AVERAGE_CALL_DURATION</th>\n",
       "      <th>REPORTED_SATISFACTION</th>\n",
       "      <th>REPORTED_USAGE_LEVEL</th>\n",
       "      <th>CONSIDERING_CHANGE_OF_PLAN</th>\n",
       "      <th>PROFITABILITY</th>\n",
       "      <th>LEAVE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one</td>\n",
       "      <td>119070.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>179158.0</td>\n",
       "      <td>541.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>very_unsat</td>\n",
       "      <td>very_little</td>\n",
       "      <td>actively_looking_into_it</td>\n",
       "      <td>3.42</td>\n",
       "      <td>LEAVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>one</td>\n",
       "      <td>40157.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>972459.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>unsat</td>\n",
       "      <td>high</td>\n",
       "      <td>considering</td>\n",
       "      <td>5.35</td>\n",
       "      <td>STAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zero</td>\n",
       "      <td>116959.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>245177.0</td>\n",
       "      <td>895.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>very_sat</td>\n",
       "      <td>very_little</td>\n",
       "      <td>actively_looking_into_it</td>\n",
       "      <td>3.51</td>\n",
       "      <td>LEAVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>one</td>\n",
       "      <td>21118.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>413655.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>very_sat</td>\n",
       "      <td>little</td>\n",
       "      <td>no</td>\n",
       "      <td>4.21</td>\n",
       "      <td>LEAVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>one</td>\n",
       "      <td>58114.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>258444.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>unsat</td>\n",
       "      <td>high</td>\n",
       "      <td>considering</td>\n",
       "      <td>3.14</td>\n",
       "      <td>LEAVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>one</td>\n",
       "      <td>54942.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>731897.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>very_unsat</td>\n",
       "      <td>high</td>\n",
       "      <td>actively_looking_into_it</td>\n",
       "      <td>6.60</td>\n",
       "      <td>STAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>zero</td>\n",
       "      <td>119820.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>863354.0</td>\n",
       "      <td>807.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>very_sat</td>\n",
       "      <td>avg</td>\n",
       "      <td>actively_looking_into_it</td>\n",
       "      <td>1.89</td>\n",
       "      <td>LEAVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>zero</td>\n",
       "      <td>107986.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>294067.0</td>\n",
       "      <td>601.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>sat</td>\n",
       "      <td>very_little</td>\n",
       "      <td>considering</td>\n",
       "      <td>5.67</td>\n",
       "      <td>LEAVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>one</td>\n",
       "      <td>117951.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>529357.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>unsat</td>\n",
       "      <td>very_little</td>\n",
       "      <td>actively_looking_into_it</td>\n",
       "      <td>7.16</td>\n",
       "      <td>LEAVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>zero</td>\n",
       "      <td>24079.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>928631.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>very_unsat</td>\n",
       "      <td>little</td>\n",
       "      <td>perhaps</td>\n",
       "      <td>5.37</td>\n",
       "      <td>STAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  COLLEGE    INCOME  OVERAGE  LEFTOVER     HOUSE  HANDSET_PRICE  \\\n",
       "0     one  119070.0     25.0      17.0  179158.0          541.0   \n",
       "1     one   40157.0     53.0       0.0  972459.0          220.0   \n",
       "2    zero  116959.0    211.0       0.0  245177.0          895.0   \n",
       "3     one   21118.0     52.0       7.0  413655.0          205.0   \n",
       "4     one   58114.0    154.0      50.0  258444.0          334.0   \n",
       "5     one   54942.0     64.0      23.0  731897.0          306.0   \n",
       "6    zero  119820.0    155.0       0.0  863354.0          807.0   \n",
       "7    zero  107986.0      0.0      54.0  294067.0          601.0   \n",
       "8     one  117951.0      0.0      29.0  529357.0          344.0   \n",
       "9    zero   24079.0      0.0       0.0  928631.0          216.0   \n",
       "\n",
       "   OVER_15MINS_CALLS_PER_MONTH  AVERAGE_CALL_DURATION REPORTED_SATISFACTION  \\\n",
       "0                          4.0                    4.0            very_unsat   \n",
       "1                         26.0                    8.0                 unsat   \n",
       "2                         15.0                   15.0              very_sat   \n",
       "3                          3.0                    5.0              very_sat   \n",
       "4                         26.0                    1.0                 unsat   \n",
       "5                          5.0                    5.0            very_unsat   \n",
       "6                         15.0                   11.0              very_sat   \n",
       "7                          4.0                    2.0                   sat   \n",
       "8                          0.0                    1.0                 unsat   \n",
       "9                          1.0                   11.0            very_unsat   \n",
       "\n",
       "  REPORTED_USAGE_LEVEL CONSIDERING_CHANGE_OF_PLAN  PROFITABILITY  LEAVE  \n",
       "0          very_little   actively_looking_into_it           3.42  LEAVE  \n",
       "1                 high                considering           5.35   STAY  \n",
       "2          very_little   actively_looking_into_it           3.51  LEAVE  \n",
       "3               little                         no           4.21  LEAVE  \n",
       "4                 high                considering           3.14  LEAVE  \n",
       "5                 high   actively_looking_into_it           6.60   STAY  \n",
       "6                  avg   actively_looking_into_it           1.89  LEAVE  \n",
       "7          very_little                considering           5.67  LEAVE  \n",
       "8          very_little   actively_looking_into_it           7.16  LEAVE  \n",
       "9               little                    perhaps           5.37   STAY  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's print the first 10 rows of the training data to see what we got\n",
    "train_orig.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing #1.1: Converting Features\n",
    "\n",
    "Sklearn - our Python library for Machine Learning - implements the CART algorithm for Decision Trees.\n",
    "That algorithm does not work with categorical or nominal features. If you try to use the algorithm as is, you will get errors.\n",
    "\n",
    "We can address this problem by converting each of these categorical features to numerical ones, and this practice is quite common. But first, we need to find out which features we must convert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COLLEGE                         object\n",
       "INCOME                         float64\n",
       "OVERAGE                        float64\n",
       "LEFTOVER                       float64\n",
       "HOUSE                          float64\n",
       "HANDSET_PRICE                  float64\n",
       "OVER_15MINS_CALLS_PER_MONTH    float64\n",
       "AVERAGE_CALL_DURATION          float64\n",
       "REPORTED_SATISFACTION           object\n",
       "REPORTED_USAGE_LEVEL            object\n",
       "CONSIDERING_CHANGE_OF_PLAN      object\n",
       "PROFITABILITY                  float64\n",
       "LEAVE                           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_orig.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above, the values associated with \"object\" as a data type are the categorical ones.\n",
    "For each of those categorical features, let's see their unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLLEGE :  ['one' 'zero']\n",
      "REPORTED_SATISFACTION :  ['very_unsat' 'unsat' 'very_sat' 'sat' 'avg']\n",
      "REPORTED_USAGE_LEVEL :  ['very_little' 'high' 'little' 'avg' 'very_high']\n",
      "CONSIDERING_CHANGE_OF_PLAN :  ['actively_looking_into_it' 'considering' 'no' 'perhaps' 'never_thought']\n",
      "LEAVE :  ['LEAVE' 'STAY']\n"
     ]
    }
   ],
   "source": [
    "# Go over the column names that are associated with \"object\" as shown above - i.e., categorical features -,\n",
    "# and for each one of them show the unique values that it takes\n",
    "for col_name in [ 'COLLEGE', 'REPORTED_SATISFACTION', 'REPORTED_USAGE_LEVEL', 'CONSIDERING_CHANGE_OF_PLAN', 'LEAVE' ]:\n",
    "    print( col_name, \": \", train_orig[col_name].unique() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the feature values\n",
    "\n",
    "Using the information that we see above, we will perform the following transformation / conversion on the features.\n",
    "\n",
    "1. For the `COLLEGE` feature, we will map the values as follows: **'one'** = 1, **'zero'** = 0\n",
    "2. For the `REPORTED_SATISFACTION`, we will map the values as follows: **'very_unsat'** = -2, **'unsat'** = -1, **'avg'** = 0, **'sat'** = 1, **'very_sat'** = 2\n",
    "3. For the `REPORTED_USAGE_LEVEL`, we will map the values as follows: **'very_little'** = -2, **'little'** = -1, **'avg'** = 0, **'high'** = 1, **'very_high'** = 2\n",
    "4. For the `CONSIDERING_CHANGE_OF_PLAN`, we will map the values as follows: **'no'** = -2, **'never_thought'** = -1, **'perhaps'** = 0, **'considering'** = 1, **'actively_looking_into_it'** = 2\n",
    "5. For the `LEAVE`, we will map the values as follows: **'LEAVE'** = 1, **'STAY'** = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames provide a very convenient way to make such replacements. Assuming a variable `my_dframe` that points to a data frame, one may simply call the method `replace()` on that data frame, like so: `my_dframe.replace(...)`.\n",
    "\n",
    "That method takes as input parameter the \"rules\" according to which the replacement will happen. These rules are given in the form of a dictionary. Each entry in that dictionary is a key value pair with the following semantics:\n",
    "* key = column name\n",
    "* value = { current_value1 : new_value1, current_value2 : new_value2, ... }\n",
    "\n",
    "Take the `COLLEGE` column as an example. In that case, the key will be `COLLEGE` because that is the name of the column with values we want to modify. The _value_ corresponding to the `COLLEGE` key, will be _another_ dictionary. In that 2nd dictionary, the key is what we already have, e.g. `one`, or `zero`, and the value (of the dictionary) is what we want to change it to, i.e., `1` and `0`.\n",
    "Putting it all together, for the `COLLEGE` example, we will have the following dictionary: `{'one' : 1, 'zero': 0}`.\n",
    "\n",
    "The rule for just this column looks like this: `{\"COLLEGE\" : {\"one\" : 1, \"zero\": 0}}`.\n",
    "The following piece of code shows all of the rules we must apply.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a dictionary mapping each string to a value\n",
    "to_replace = {'COLLEGE':{'one':1,'zero':0},\n",
    "           'REPORTED_SATISFACTION':{'very_unsat':-2,'unsat':-1,'avg':0,'sat':1,'very_sat':2},\n",
    "           'REPORTED_USAGE_LEVEL':{'very_little':-2,'little':-1,'avg':0,'high':1,'very_high':2},\n",
    "           'CONSIDERING_CHANGE_OF_PLAN':{'no':-2,'never_thought':-1,'perhaps':0,'considering':1,'actively_looking_into_it':2},\n",
    "           'LEAVE':{'LEAVE':1,'STAY':0}\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `replace()` method returns a new data frame where the modifications have already taken place.\n",
    "We want to apply these transformations to **both** the train set **and** the test set.\n",
    "The code below performs those changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transformations to both the training and the testing dataset\n",
    "train_data = train_orig.replace(to_replace)\n",
    "test_data = test_orig.replace(to_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COLLEGE</th>\n",
       "      <th>INCOME</th>\n",
       "      <th>OVERAGE</th>\n",
       "      <th>LEFTOVER</th>\n",
       "      <th>HOUSE</th>\n",
       "      <th>HANDSET_PRICE</th>\n",
       "      <th>OVER_15MINS_CALLS_PER_MONTH</th>\n",
       "      <th>AVERAGE_CALL_DURATION</th>\n",
       "      <th>REPORTED_SATISFACTION</th>\n",
       "      <th>REPORTED_USAGE_LEVEL</th>\n",
       "      <th>CONSIDERING_CHANGE_OF_PLAN</th>\n",
       "      <th>PROFITABILITY</th>\n",
       "      <th>LEAVE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>119070.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>179158.0</td>\n",
       "      <td>541.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>40157.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>972459.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>116959.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>245177.0</td>\n",
       "      <td>895.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>21118.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>413655.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>4.21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>58114.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>258444.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   COLLEGE    INCOME  OVERAGE  LEFTOVER     HOUSE  HANDSET_PRICE  \\\n",
       "0        1  119070.0     25.0      17.0  179158.0          541.0   \n",
       "1        1   40157.0     53.0       0.0  972459.0          220.0   \n",
       "2        0  116959.0    211.0       0.0  245177.0          895.0   \n",
       "3        1   21118.0     52.0       7.0  413655.0          205.0   \n",
       "4        1   58114.0    154.0      50.0  258444.0          334.0   \n",
       "\n",
       "   OVER_15MINS_CALLS_PER_MONTH  AVERAGE_CALL_DURATION  REPORTED_SATISFACTION  \\\n",
       "0                          4.0                    4.0                     -2   \n",
       "1                         26.0                    8.0                     -1   \n",
       "2                         15.0                   15.0                      2   \n",
       "3                          3.0                    5.0                      2   \n",
       "4                         26.0                    1.0                     -1   \n",
       "\n",
       "   REPORTED_USAGE_LEVEL  CONSIDERING_CHANGE_OF_PLAN  PROFITABILITY  LEAVE  \n",
       "0                    -2                           2           3.42      1  \n",
       "1                     1                           1           5.35      0  \n",
       "2                    -2                           2           3.51      1  \n",
       "3                    -1                          -2           4.21      1  \n",
       "4                     1                           1           3.14      1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the changes have been correctly applied\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing #1.2: From DataFrame to _Features_ and _Labels_\n",
    "\n",
    "The dataframe has everything together: our features of interest and the value for the target variable -- what we often call _\"labels\"_. As we've already seen, when training a model, our classifier expects the _features_ separately from the _labels_.\n",
    "\n",
    "In the [fit()](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.fit) method, the first argument corresponds to the features only, while the labels (or target values) are the second argument.\n",
    "\n",
    "For that reason, it is very convenient to separate the features from the labels for a given dataframe.\n",
    "\n",
    "Implement a simple method below that is given a dataframe as input and returns 2 items: the first one will be the _features_ of the input dataframe, while the second one will be the _labels_ (column) of that dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COLLEGE', 'INCOME', 'OVERAGE', 'LEFTOVER', 'HOUSE', 'HANDSET_PRICE', 'OVER_15MINS_CALLS_PER_MONTH', 'AVERAGE_CALL_DURATION', 'REPORTED_SATISFACTION', 'REPORTED_USAGE_LEVEL', 'CONSIDERING_CHANGE_OF_PLAN', 'PROFITABILITY']\n"
     ]
    }
   ],
   "source": [
    "print(list(train_data.keys())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the method, following the description above\n",
    "# Given a dataframe, return the features and the labels (target variable values)\n",
    "\n",
    "def separate_features_and_labels(dframe):\n",
    "    variable_names = list(dframe.keys())[:-1]\n",
    "    target_name = \"LEAVE\"\n",
    "    \n",
    "    features = dframe[variable_names]   # Your code here to get the features\n",
    "    labels = dframe[target_name]   # Your code here to get the labels\n",
    "    return features, labels  # This method returns the features and the labels in that order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       COLLEGE    INCOME  OVERAGE  LEFTOVER     HOUSE  HANDSET_PRICE  \\\n",
      "0            1  119070.0     25.0      17.0  179158.0          541.0   \n",
      "1            1   40157.0     53.0       0.0  972459.0          220.0   \n",
      "2            0  116959.0    211.0       0.0  245177.0          895.0   \n",
      "3            1   21118.0     52.0       7.0  413655.0          205.0   \n",
      "4            1   58114.0    154.0      50.0  258444.0          334.0   \n",
      "5            1   54942.0     64.0      23.0  731897.0          306.0   \n",
      "6            0  119820.0    155.0       0.0  863354.0          807.0   \n",
      "7            0  107986.0      0.0      54.0  294067.0          601.0   \n",
      "8            1  117951.0      0.0      29.0  529357.0          344.0   \n",
      "9            0   24079.0      0.0       0.0  928631.0          216.0   \n",
      "10           1   29006.0    219.0      83.0  557407.0          162.0   \n",
      "11           1   73329.0     52.0      43.0  430486.0          371.0   \n",
      "12           0  111572.0      0.0      78.0  254936.0          847.0   \n",
      "13           1   66065.0      0.0       0.0  490357.0          280.0   \n",
      "14           0   73102.0     83.0       8.0  355803.0          375.0   \n",
      "15           1   21944.0      0.0      27.0  954543.0          177.0   \n",
      "16           0   64165.0      0.0       0.0  638740.0          335.0   \n",
      "17           1   32242.0      0.0       0.0  634253.0          193.0   \n",
      "18           0   44083.0    208.0       0.0  262710.0          173.0   \n",
      "19           1   76435.0    199.0       0.0  290126.0          388.0   \n",
      "20           1   64838.0     87.0      41.0  665783.0          353.0   \n",
      "21           1   30186.0      0.0      13.0  558518.0          144.0   \n",
      "22           1   32870.0    208.0      20.0  221601.0          224.0   \n",
      "23           1  140800.0     58.0      19.0  719481.0          423.0   \n",
      "24           0  114634.0    201.0       0.0  435784.0          819.0   \n",
      "25           0  120319.0     55.0       0.0  344396.0          734.0   \n",
      "26           0   23661.0     66.0      11.0  179152.0          235.0   \n",
      "27           1  103801.0     45.0       5.0  858691.0          818.0   \n",
      "28           1  142073.0      0.0       0.0  584762.0          482.0   \n",
      "29           1  107293.0    199.0      13.0  568557.0          461.0   \n",
      "...        ...       ...      ...       ...       ...            ...   \n",
      "14970        0   33939.0    178.0      54.0  185593.0          177.0   \n",
      "14971        0  150525.0     82.0       0.0  926836.0          748.0   \n",
      "14972        0  146748.0      0.0      42.0  229795.0          645.0   \n",
      "14973        0   30128.0      0.0      72.0  641262.0          148.0   \n",
      "14974        1   35882.0     55.0      19.0  815343.0          560.0   \n",
      "14975        0  106459.0     92.0      43.0  856157.0          569.0   \n",
      "14976        1   73560.0    152.0      11.0  932243.0          272.0   \n",
      "14977        0   66920.0     53.0       0.0  772819.0          360.0   \n",
      "14978        0   32107.0      0.0      74.0  224848.0          175.0   \n",
      "14979        1   84463.0      0.0      79.0  374066.0          393.0   \n",
      "14980        1   79439.0     77.0       0.0  599096.0          267.0   \n",
      "14981        0   62554.0     79.0      33.0  259391.0          255.0   \n",
      "14982        0  133122.0     59.0      21.0  717603.0          762.0   \n",
      "14983        1  136344.0      0.0       0.0  560049.0          532.0   \n",
      "14984        1   76921.0    150.0      88.0  966825.0          281.0   \n",
      "14985        0   32631.0     73.0      24.0  906043.0          288.0   \n",
      "14986        0   31026.0     44.0       0.0  641320.0          156.0   \n",
      "14987        1   22564.0     75.0       5.0  173132.0          200.0   \n",
      "14988        1  148369.0      0.0      57.0  183684.0          587.0   \n",
      "14989        0  104080.0    182.0       0.0  153140.0          555.0   \n",
      "14990        0  148504.0     77.0      11.0  164489.0          271.0   \n",
      "14991        1   50048.0    160.0      38.0  166571.0          381.0   \n",
      "14992        0   40039.0    182.0      61.0  823088.0          137.0   \n",
      "14993        1   90976.0     83.0      21.0  950415.0          298.0   \n",
      "14994        0   27920.0    218.0      82.0  228157.0          209.0   \n",
      "14995        0  111474.0      0.0      13.0  755810.0          871.0   \n",
      "14996        0   92090.0      0.0       0.0  792073.0          290.0   \n",
      "14997        0   81098.0     85.0      25.0  796984.0          269.0   \n",
      "14998        1   72327.0      0.0      46.0  646786.0          301.0   \n",
      "14999        1   37211.0    185.0      29.0  812497.0          208.0   \n",
      "\n",
      "       OVER_15MINS_CALLS_PER_MONTH  AVERAGE_CALL_DURATION  \\\n",
      "0                              4.0                    4.0   \n",
      "1                             26.0                    8.0   \n",
      "2                             15.0                   15.0   \n",
      "3                              3.0                    5.0   \n",
      "4                             26.0                    1.0   \n",
      "5                              5.0                    5.0   \n",
      "6                             15.0                   11.0   \n",
      "7                              4.0                    2.0   \n",
      "8                              0.0                    1.0   \n",
      "9                              1.0                   11.0   \n",
      "10                            28.0                    1.0   \n",
      "11                             3.0                    2.0   \n",
      "12                             1.0                    1.0   \n",
      "13                             1.0                    8.0   \n",
      "14                             5.0                    4.0   \n",
      "15                             1.0                    2.0   \n",
      "16                             1.0                   14.0   \n",
      "17                             0.0                   10.0   \n",
      "18                            21.0                    9.0   \n",
      "19                            17.0                    9.0   \n",
      "20                             5.0                    2.0   \n",
      "21                             0.0                   11.0   \n",
      "22                            27.0                    5.0   \n",
      "23                             5.0                    2.0   \n",
      "24                            20.0                   14.0   \n",
      "25                             3.0                   10.0   \n",
      "26                             3.0                    4.0   \n",
      "27                             3.0                    5.0   \n",
      "28                             0.0                   10.0   \n",
      "29                            12.0                    5.0   \n",
      "...                            ...                    ...   \n",
      "14970                          3.0                    1.0   \n",
      "14971                          3.0                   12.0   \n",
      "14972                          0.0                    1.0   \n",
      "14973                          1.0                    1.0   \n",
      "14974                          4.0                   11.0   \n",
      "14975                          5.0                    2.0   \n",
      "14976                         21.0                    4.0   \n",
      "14977                          5.0                   12.0   \n",
      "14978                          1.0                    1.0   \n",
      "14979                          1.0                    2.0   \n",
      "14980                          3.0                    8.0   \n",
      "14981                          3.0                    2.0   \n",
      "14982                          5.0                    5.0   \n",
      "14983                          1.0                   11.0   \n",
      "14984                         25.0                    2.0   \n",
      "14985                          3.0                    4.0   \n",
      "14986                         25.0                   10.0   \n",
      "14987                          3.0                    5.0   \n",
      "14988                          0.0                    2.0   \n",
      "14989                         17.0                   11.0   \n",
      "14990                          4.0                    6.0   \n",
      "14991                         16.0                    2.0   \n",
      "14992                         26.0                    2.0   \n",
      "14993                          5.0                    6.0   \n",
      "14994                         18.0                    4.0   \n",
      "14995                          0.0                    4.0   \n",
      "14996                          1.0                    8.0   \n",
      "14997                          3.0                    1.0   \n",
      "14998                          0.0                    1.0   \n",
      "14999                         26.0                    1.0   \n",
      "\n",
      "       REPORTED_SATISFACTION  REPORTED_USAGE_LEVEL  \\\n",
      "0                         -2                    -2   \n",
      "1                         -1                     1   \n",
      "2                          2                    -2   \n",
      "3                          2                    -1   \n",
      "4                         -1                     1   \n",
      "5                         -2                     1   \n",
      "6                          2                     0   \n",
      "7                          1                    -2   \n",
      "8                         -1                    -2   \n",
      "9                         -2                    -1   \n",
      "10                        -2                    -2   \n",
      "11                         0                     2   \n",
      "12                        -2                    -1   \n",
      "13                        -2                    -1   \n",
      "14                        -2                     2   \n",
      "15                        -2                    -1   \n",
      "16                        -1                     1   \n",
      "17                         2                    -1   \n",
      "18                        -2                    -1   \n",
      "19                        -2                    -2   \n",
      "20                        -1                     0   \n",
      "21                        -2                     1   \n",
      "22                         1                    -1   \n",
      "23                         0                    -2   \n",
      "24                         2                    -1   \n",
      "25                         2                     2   \n",
      "26                        -2                    -1   \n",
      "27                         0                     2   \n",
      "28                        -2                    -1   \n",
      "29                         2                    -2   \n",
      "...                      ...                   ...   \n",
      "14970                     -2                    -1   \n",
      "14971                      2                     1   \n",
      "14972                     -1                    -2   \n",
      "14973                      2                    -1   \n",
      "14974                      2                    -2   \n",
      "14975                     -1                    -1   \n",
      "14976                      2                     2   \n",
      "14977                     -1                     2   \n",
      "14978                      2                    -1   \n",
      "14979                     -2                     2   \n",
      "14980                      2                    -1   \n",
      "14981                     -2                    -1   \n",
      "14982                     -2                    -1   \n",
      "14983                     -2                    -1   \n",
      "14984                     -1                     2   \n",
      "14985                      2                     1   \n",
      "14986                     -2                     2   \n",
      "14987                     -2                    -1   \n",
      "14988                      2                     2   \n",
      "14989                      0                    -1   \n",
      "14990                     -2                     2   \n",
      "14991                     -2                     1   \n",
      "14992                     -2                     2   \n",
      "14993                      0                    -1   \n",
      "14994                     -2                     2   \n",
      "14995                      0                     1   \n",
      "14996                      0                     2   \n",
      "14997                      2                    -1   \n",
      "14998                     -2                     2   \n",
      "14999                      0                     0   \n",
      "\n",
      "       CONSIDERING_CHANGE_OF_PLAN  PROFITABILITY  \n",
      "0                               2           3.42  \n",
      "1                               1           5.35  \n",
      "2                               2           3.51  \n",
      "3                              -2           4.21  \n",
      "4                               1           3.14  \n",
      "5                               2           6.60  \n",
      "6                               2           1.89  \n",
      "7                               1           5.67  \n",
      "8                               2           7.16  \n",
      "9                               0           5.37  \n",
      "10                             -2           1.76  \n",
      "11                              1           7.03  \n",
      "12                              1           7.83  \n",
      "13                              2           2.79  \n",
      "14                              1           1.82  \n",
      "15                              1           4.74  \n",
      "16                              2           6.91  \n",
      "17                             -2           6.36  \n",
      "18                              0           3.98  \n",
      "19                              1           2.34  \n",
      "20                              1           5.56  \n",
      "21                             -1           4.31  \n",
      "22                              1           3.82  \n",
      "23                              1           4.78  \n",
      "24                              1           3.19  \n",
      "25                             -1           4.53  \n",
      "26                              2           3.89  \n",
      "27                              2           3.94  \n",
      "28                              1           5.00  \n",
      "29                              1           3.55  \n",
      "...                           ...            ...  \n",
      "14970                          -2           4.03  \n",
      "14971                          -2           3.80  \n",
      "14972                           1           7.03  \n",
      "14973                          -2           7.60  \n",
      "14974                           1           7.54  \n",
      "14975                          -1           6.01  \n",
      "14976                          -2           7.03  \n",
      "14977                          -1           6.09  \n",
      "14978                           1           7.05  \n",
      "14979                           1           6.20  \n",
      "14980                          -2           3.76  \n",
      "14981                          -2           6.77  \n",
      "14982                          -2           5.11  \n",
      "14983                           1           2.38  \n",
      "14984                          -2           6.71  \n",
      "14985                           1           6.85  \n",
      "14986                           2           6.08  \n",
      "14987                          -2           1.89  \n",
      "14988                          -2           6.74  \n",
      "14989                          -1           4.15  \n",
      "14990                          -2           2.97  \n",
      "14991                           2           2.85  \n",
      "14992                           0           7.69  \n",
      "14993                          -1           5.94  \n",
      "14994                           2           2.11  \n",
      "14995                           2           4.67  \n",
      "14996                          -1           5.82  \n",
      "14997                          -1           5.86  \n",
      "14998                           2           6.04  \n",
      "14999                           2           6.67  \n",
      "\n",
      "[15000 rows x 12 columns]\n",
      "============================\n",
      "0        1\n",
      "1        0\n",
      "2        1\n",
      "3        1\n",
      "4        1\n",
      "5        0\n",
      "6        1\n",
      "7        1\n",
      "8        1\n",
      "9        0\n",
      "10       1\n",
      "11       1\n",
      "12       0\n",
      "13       0\n",
      "14       0\n",
      "15       1\n",
      "16       0\n",
      "17       0\n",
      "18       1\n",
      "19       1\n",
      "20       1\n",
      "21       1\n",
      "22       1\n",
      "23       0\n",
      "24       1\n",
      "25       1\n",
      "26       1\n",
      "27       0\n",
      "28       0\n",
      "29       1\n",
      "        ..\n",
      "14970    1\n",
      "14971    1\n",
      "14972    1\n",
      "14973    0\n",
      "14974    1\n",
      "14975    1\n",
      "14976    0\n",
      "14977    0\n",
      "14978    1\n",
      "14979    0\n",
      "14980    0\n",
      "14981    0\n",
      "14982    0\n",
      "14983    0\n",
      "14984    0\n",
      "14985    0\n",
      "14986    0\n",
      "14987    1\n",
      "14988    1\n",
      "14989    1\n",
      "14990    0\n",
      "14991    0\n",
      "14992    0\n",
      "14993    0\n",
      "14994    1\n",
      "14995    1\n",
      "14996    1\n",
      "14997    0\n",
      "14998    0\n",
      "14999    0\n",
      "Name: LEAVE, Length: 15000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# In case you want to test your method\n",
    "X,Y=separate_features_and_labels(train_data)\n",
    "print(X)\n",
    "print(\"============================\")\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable #2: Accuracy and Cross-validation\n",
    "\n",
    "For the following questions, train and (subsequently) use a Decision Tree classifier.\n",
    "The classifier must use the `entropy` criterion to split.\n",
    "For now, **do not** specify the `max_depth` parameter.\n",
    "\n",
    "\n",
    "Following the above instructions, compute and report the following:\n",
    "1. Classifier Accuracy on the training data <br/> <br/>\n",
    "\n",
    "2. Classifier Accuracy on the test data <br/> <br/>\n",
    "\n",
    "3. Classifier Average Accuracy of a **10-fold** cross-validation. Check the `cv` parameter of the `model_selection.cross_val_score()` method. <br/> <br/>\n",
    "\n",
    "4. Train your classifier on 66% of the trainnig data and report the accuracy on the remaining 34%. Use the **numerical part** of your _NetID_ to randomize the instance selection process. Check the [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) method to do this step easily, and pay attention to the `random_state` parameter of that method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><div style=\"color:red\">COMMENTS:</div></b>\n",
    "\n",
    "* In your `model_acc()`, `cv()` and `split_train()` methods, you have named the parameter `model`. To someone else who is reading your code (like me), this gives the impression you are talking about a machine learning model (e.g., a Decision Tree, a Logistic Regression model, etc). This isn't the case though. You are providing **data** upon which you'll train your model (your DecisionTree).\n",
    "\n",
    "\n",
    "* Since you are already passing an integer for the `fold` parameter, there is no need to be doing `int(fold)`.\n",
    "\n",
    "\n",
    "* To report the accuracy on the test set, you should be training on the features and labels coming from the `train_data` and then use the `predict()` and `metrics.accuracy_score()` methods on the `test_data`.\n",
    "\n",
    "\n",
    "* When doing cross validation, you should only perform it on the `train_data` - this is how we perform cross-validation, in general. Assuming you wanted to treat both datasets together - not requested by the question and not how we use the train / test split in data mining projects, in general - you should first combine the two datasets into a single dataset and then apply Cross Validation on the big dataset.\n",
    "\n",
    "\n",
    "* In the `split_train()` method implementation, you are doing `tree_clf.fit(X_test, Y_test)`, meaning that you are training your model on the 34% part of the `features` and `labels` and then using again that 34% to compute the accuracy.\n",
    "\n",
    "Check the answer key for more details and how to approach this question. If the above do not make sense, come and see me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on the training data : 1.0000\n",
      "accuracy on the test data     : 1.0000\n",
      "accuracy on the 10-fold CV    : 1.2260\n",
      "accuracy on the 66% split    : 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashtsoi/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def model_acc(model):\n",
    "    features, labels = separate_features_and_labels(model)\n",
    "    tree_clf = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "    tree_clf.fit(features, labels)\n",
    "    acc = metrics.accuracy_score(tree_clf.predict(features), labels)\n",
    "    return acc\n",
    "    \n",
    "# Implement the following\n",
    "\n",
    "# accuracy on the training data:\n",
    "acc_train = model_acc(train_data)\n",
    "\n",
    "# accuracy on the test data:\n",
    "acc_test = model_acc(test_data)\n",
    "\n",
    "# average (mean) accuracy on 10-fold CV:\n",
    "def cv(model,fold):\n",
    "    features, labels = separate_features_and_labels(model)\n",
    "    tree_clf = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "    tree_clf.fit(features, labels)\n",
    "    return cross_val_score(tree_clf, features, labels, cv=int(fold))\n",
    "\n",
    "acc_10cv = np.mean(cv(train_data,10)+cv(test_data,10))\n",
    "\n",
    "# accuracy on 66% split:\n",
    "def split_train(model):\n",
    "    features, labels = separate_features_and_labels(model)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(features, labels, train_size=0.66, random_state=418)\n",
    "    tree_clf = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "    tree_clf.fit(X_test,Y_test)\n",
    "    return metrics.accuracy_score(tree_clf.predict(X_test), Y_test)\n",
    "\n",
    "acc_66pct = split_train(train_data)\n",
    "\n",
    "\n",
    "print(\"accuracy on the training data : %.4f\" % acc_train)\n",
    "print(\"accuracy on the test data     : %.4f\" % acc_test)\n",
    "print(\"accuracy on the 10-fold CV    : %.4f\" % acc_10cv)\n",
    "print(\"accuracy on the 66%% split    : %.4f\" % acc_66pct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable #2.1: Complexity Control\n",
    "\n",
    "We've said in class that Decision Trees can overfit if we allow them to increase in depth a lot, and we should plot their performance (accuracy) as a function of their depth to figure out the \"sweet spot\".\n",
    "\n",
    "We also mentioned that this classifier takes into account several other conditions, not just the depth, to check if they should split a region further. One such condition is the minimum number of instances we allow to appear in a leaf node. In sklearn's `DecisionTreeClassifier` this parameter is called `min_samples_leaf` and is specified when _creating_ the classifier, in a way similar to specifying the `max_depth` parameter. For more details check [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "\n",
    "For this deliverable, you will pick several different values (at least 5) for the depth of the tree and several different values (at least 4) for the minimum number of leaf instances. For each combination of (`max_depth`, `min_samples_leaf`) you will compute the average accuracy of a 3-fold cross validation.\n",
    "\n",
    "You will then visualize, in the same plot, the different 3-fold average accuracy scores (y-axis) that you get as a function of the tree depth (x-axis). Each value of `min_samples_leaf` will be a separate line in the plot that you generate.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Requirements Checklist**\n",
    "* Select several values (at least 5) for the maximum depth of the tree.\n",
    "* Select several values (at least 4) for the minimum number of leaf instances\n",
    "* For each combination of (max_depth, min_samples_leaf):\n",
    "    * Do 3-fold cross validation (CV)\n",
    "    * Compute the average accuracy of the 3-fold CV process. That's the accuracy score for this combination\n",
    "* Visualize the computed information as follows:\n",
    "    * The x axis must be the maximum tree depth values (in ascending order)\n",
    "    * The y axis must be the accuracy score\n",
    "    * Each value of minimum instances at a leaf needs to be a separate line in the plot\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "An easy way to generate the necessary values is through a **nested for-loop**.\n",
    "We can then store these results in a DataFrame and plot them.\n",
    "A simplified example is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'This should be accuracy')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample code that generates y-axis values for a given combination of 2 variables.\n",
    "# This code is provided for convenience\n",
    "\n",
    "xvals = range(1, 10)\n",
    "line_vals = range(1, 4)\n",
    "\n",
    "# We will use a dataframe because it has a simple-to-use plotting function.\n",
    "sample_df = pd.DataFrame(index=xvals, columns=line_vals)\n",
    "\n",
    "for v in line_vals:  # This loop controls the different lines\n",
    "    l = []\n",
    "    for x in xvals:  # This loop controls the different values on the x-axis\n",
    "        l.append(x ** v)  # Simply compute x^v. Append in the list your value for the y-axis\n",
    "\n",
    "    sample_df[v] = l  #Store the result to the dataframe for the particular line\n",
    "\n",
    "# We can plot the contents of a dataframe with the plot() method\n",
    "ax = sample_df.plot()\n",
    "ax.set_xlabel(\"This should be tree depth\")\n",
    "ax.set_ylabel(\"This should be accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1   2    3\n",
      "1  1   1    1\n",
      "2  2   4    8\n",
      "3  3   9   27\n",
      "4  4  16   64\n",
      "5  5  25  125\n",
      "6  6  36  216\n",
      "7  7  49  343\n",
      "8  8  64  512\n",
      "9  9  81  729\n"
     ]
    }
   ],
   "source": [
    "print(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assignment asked that, given a combination of depth and minimum instances at a leaf, you will perform a 3-fold cross validation and compute the average score for that.\n",
    "\n",
    "Things will be a lot easier if you create a method that:\n",
    "* Takes the necessary input arguments\n",
    "* Trains a DecisionTreeClassifier on the proper input\n",
    "* Runs a 3-fold cross validation\n",
    "* Computes the average accuracy of these 3 folds\n",
    "* Returns the average accuracy\n",
    "\n",
    "_Note:_ You do not have to structure your code this way if you do not want to, as long as you meet the objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><div style=\"color:red\">COMMENTS:</div></b>\n",
    "\n",
    "It'd be better to also pass the dataset you are training on as a parameter - that would be the `train_data` here. The reason is that your current implementation, there is an implicit dependency between what is happening inside the method and what is going on outside.\n",
    "\n",
    "Also, for cross validation, there is no need to do `.fit()` on the model in advance. The `cross_val_score()` method will internally call the `fit()` for the proper splits / iterations of your data.\n",
    "\n",
    "You can make things simpler and return the average value right away, instead of just returning the results of `cross_val_score()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Think about the parameters that you will need for your code. Add them as arguments to your method\n",
    "def cv_eval(depth,leaf):\n",
    "    features, labels = separate_features_and_labels(train_data)\n",
    "    tree_clf = DecisionTreeClassifier(max_depth=depth, min_samples_leaf=leaf, criterion=\"entropy\")\n",
    "    tree_clf.fit(features, labels)\n",
    "    return cross_val_score(tree_clf, features, labels, cv=3)  # Return the proper value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that you must select a number of depth levels and a number of minimum instances for the leaf nodes. You can then structure your code according to the sample code provided earlier.\n",
    "\n",
    "You can then use the method that you created above to compute the average 3-fold Cross Validation accuracy score as your y-value for a given combination of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1         10        50        100       150\n",
      "1  0.644200  0.644200  0.644200  0.644200  0.644200\n",
      "2  0.653733  0.653733  0.653733  0.653733  0.653733\n",
      "3  0.678667  0.678667  0.678667  0.678667  0.678467\n",
      "4  0.703867  0.703867  0.703800  0.703867  0.702400\n",
      "5  0.705934  0.706068  0.706534  0.705601  0.704000\n",
      "6  0.702134  0.702534  0.702134  0.701801  0.699534\n",
      "7  0.699001  0.698401  0.702400  0.701534  0.698267\n",
      "8  0.691267  0.692667  0.700667  0.700067  0.698267\n",
      "9  0.685668  0.689401  0.695934  0.700201  0.698267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "max_tree_depth = 10\n",
    "leaf_instance = [1,10,50,100,150]\n",
    "df = pd.DataFrame(index=range(1,int(max_tree_depth)), columns=leaf_instance)\n",
    "for li in leaf_instance:  # This loop controls the different values on the x-axis\n",
    "    scores=[]\n",
    "    for td in range(1,int(max_tree_depth)):\n",
    "        scores.append(np.mean(cv_eval(td,li)))\n",
    "    df[li] = scores\n",
    "print(df)\n",
    "\n",
    "# We can plot the contents of a dataframe with the plot() method\n",
    "ax = df.plot()\n",
    "ax.set_xlabel(\"Tree depth\")\n",
    "ax.set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable #2.2: Evaluation\n",
    "\n",
    "Based on the graph that you generated above:\n",
    "- What value would you use for max depth?\n",
    "- What value would you use for min leaf size?\n",
    "- If you could only specify one, which complexity parameter would you choose (max depth or min leaf size)? Why ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "We can see from the graph that the accuracy increases as depth increases, but after depth 5, the accuracy of all models start to decrease. Therefore, we should use ***depth 5*** for the model. In terms of min leaf size, it is better to use somewhere ***around 50***, since the green line (model with 50 min leaf size) has the highest accuracy at tree depth 5.\n",
    "\n",
    "If I were to only choose between specifying the tree depth or the leaf size, I would choose the ***tree depth***. This is because if we compare accross accuracy of models wih different tree depth, models with different tree depths vary significantly. However, it seems that no matter what minimum leaf size the model has, the accuracy does not vary as much. In other words, using minimum leaf size gives us less information gain in a way that using minimum leaf size to separate out the accuracy of the models does not decrease the entropy of each \"categories\" (e.g. low leaf size, medium leaf size, high leaf size, etc.) of models.\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><div style=\"color:red\">COMMENTS:</div></b>\n",
    "\n",
    "Good answer overall!\n",
    "\n",
    "Technically speaking, leaf size doesn't give \"more\" or \"less\" information gain. It implicitly allows or prevents the tree to grow deeper. Make sure you don't confuse those concepts. If the distinction is not clear, come and find me to discuss this.\n",
    "\n",
    "Also, when you say \"around 50\", would you recommend that we pick 47 or 51? These values are also \"around 50\". Generally, \"around 50\" implies a \"range\" of values that we are allowed to pick, leading the others to think that you _actually tried_ those other values (around 50), and they gave consistent results.  Just keep this in mind for future reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable #2.3: Test set Evaluation\n",
    "\n",
    "Train a DecisionTreeClassifier using the parameters combination that produced the best result according to your complexity control evaluation. Report the accuracy of that classifier on the _test_ dataset, i.e. the 'churn_test.csv' dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><div style=\"color:red\">COMMENTS:</div></b>\n",
    "\n",
    "As with the previous case, you are learning a model on some data and then computing the accuracy on the same data. This way, you are overestimating your model's performance.\n",
    "\n",
    "Also, you have redefined a method that you already had: `model_acc()`. It's better to use an entirely new name, otherwise your earlier code may stop working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Accuracy on the test data     : 0.7094\n"
     ]
    }
   ],
   "source": [
    "# Show work here\n",
    "def model_acc(model,depth,leaf):\n",
    "    features, labels = separate_features_and_labels(model)\n",
    "    tree_clf = DecisionTreeClassifier(max_depth=depth, min_samples_leaf=leaf, criterion=\"entropy\")\n",
    "    tree_clf.fit(features, labels)\n",
    "    acc = metrics.accuracy_score(tree_clf.predict(features), labels)\n",
    "    return acc\n",
    "\n",
    "best_depth = 5\n",
    "best_leaf_size = 50\n",
    "acc_test = model_acc(test_data,best_depth,best_leaf_size)\n",
    "\n",
    "print(\"Best Model Accuracy on the test data     : %.4f\" % acc_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable #2.3: Learning Curves\n",
    "\n",
    "Learning curves determine how much data you realistically need to train your model. You will be picking a portion of your original training data to train your model and then testing it on other datasets. In particular:\n",
    "\n",
    "* Use the parameters that you picked as your best performing ones in your complexity control.\n",
    "* Select multiple (at least 5, non-zero) percentage values. Each such percentage corresponds to how much of your original data you'll use for training.\n",
    "* Use `train_test_split()` to select a percentage of the training data to use to fit the model\n",
    "* Compute the accuracy on:\n",
    "    * The train set that you got from the `train_test_split()` method.\n",
    "    * 3-fold cross validation on the train set that you got from the split. You can / should reuse your method from before.\n",
    "    * The test set that you got from the `train_test_split()` method.\n",
    "    * The original testing dataset (the one that was given to us and is stored in 'data/churn_test.csv')\n",
    "* Plot the accuracy (y-axis) as computed above VS train size (x-axis), for each of the 4 cases\n",
    "\n",
    "\n",
    "Start by computing the above once for each percentage value. However, for a given percentage value, you should repeat these computations multiple times and report their average, to get more robust results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><div style=\"color:red\">COMMENTS:</div></b>\n",
    "\n",
    "Basically, you've run into the problem that I pointed out earlier: you named your parameter \"model\" whereas it corresponds to the **data** that you are passing.  So, make sure that you pick names that minimize these kind of errors.\n",
    "\n",
    "In addition, your most recent `model_acc()` method accepts 3 parameters which are:\n",
    "1. The **data** (dataframe) to separate and train on\n",
    "1. The **best max depth** to train the decision tree\n",
    "1. The **best leaf size** to train the decision tree\n",
    "\n",
    "In your code below, you are passing 3 entirely different parameters, which, of course, causes Python to complain.\n",
    "\n",
    "First, you'll need to revisit some of the earlier comments, about what you are using to train your classifier and what you are using to measure the accuracy. Once you've corrected those, you can then use your code below by passing the right parameters.\n",
    "\n",
    "If you get stuck with such issues, or even if you don't understand the comments I've made here, you are always welcome to come and see me to resolve them.\n",
    "\n",
    "Also, check the answer key for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DecisionTreeClassifier' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8300530ec9a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Write your code to generate the requested accuracy scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mdf_lc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_lc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Make sure that you keep track of them so that you can visualize them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-8300530ec9a4>\u001b[0m in \u001b[0;36macc\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtree_clf_lc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_clf_lc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_clf_lc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_clf_lc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-6f88b395b3e5>\u001b[0m in \u001b[0;36mmodel_acc\u001b[0;34m(model, depth, leaf)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Show work here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodel_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mleaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseparate_features_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtree_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"entropy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtree_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-07b341ed9e36>\u001b[0m in \u001b[0;36mseparate_features_and_labels\u001b[0;34m(dframe)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mseparate_features_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtarget_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LEAVE\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DecisionTreeClassifier' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Your code here\n",
    "splits = list(s/10 for s in range(10))\n",
    "datas = ['train_data','cv','test_data','og']\n",
    "df_lc = pd.DataFrame(index=splits, columns = datas)\n",
    "\n",
    "tree_clf_lc = DecisionTreeClassifier(max_depth=best_depth, min_samples_leaf=best_leaf_size, criterion=\"entropy\")\n",
    "\n",
    "Xtest,Ytest = separate_features_and_labels(test_data)\n",
    "Xtrain,Ytrain = separate_features_and_labels(train_data)\n",
    "    \n",
    "# Select the different training sizes\n",
    "def acc(split):\n",
    "    a = []\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(Xtrain, Ytrain, test_size=split)\n",
    "    tree_clf_lc.fit(X_train,Y_train)\n",
    "    a.append(model_acc(tree_clf_lc,X_train,Y_train))\n",
    "    a.append(np.mean(cross_val_score(tree_clf_lc,X_train,Y_train,cv=3)))\n",
    "    a.append(model_acc(tree_clf_lc,X_test,Y_test))\n",
    "    \n",
    "    a.append(model_accuracy(tree_clf_lc,Xtest,Ytest))\n",
    "    return a\n",
    "# Write your code to generate the requested accuracy scores.\n",
    "for s in splits:\n",
    "    df_lc.loc[s] = acc(s)\n",
    "print(df_lc)\n",
    "# Make sure that you keep track of them so that you can visualize them.\n",
    "plot_lc = df_lc.plot()\n",
    "plot_lc.set_xlabel['Splits']\n",
    "plot_lc.set_ylabel['Accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable #3.1: Evaluation\n",
    "Would you recommend your firm spend money to collect data on more customers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "Your answer here\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
